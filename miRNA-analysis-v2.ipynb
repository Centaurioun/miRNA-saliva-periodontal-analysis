{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miRNA Analysis v2\n",
    "\n",
    "This notebook performs miRNA analysis with updated methods and additional steps as outlined in the miRNA-Manuscript-Revision.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data per column:\n",
      "GROUP                      0\n",
      "SEX                        0\n",
      "AGE                        0\n",
      "plaque_index               0\n",
      "gingival_index             0\n",
      "pocket_depth               0\n",
      "bleeding_on_probing        0\n",
      "number_of_missing_teeth    0\n",
      "mean_mir146a               0\n",
      "mean_mir146b               0\n",
      "mean_mir155                0\n",
      "mean_mir203                0\n",
      "mean_mir223                0\n",
      "mean_mir381p               0\n",
      "mean_GAPDH                 0\n",
      "dtype: int64\n",
      "No missing values found.\n",
      "Data types after conversion:\n",
      "GROUP                      category\n",
      "SEX                        category\n",
      "AGE                           int64\n",
      "plaque_index                float64\n",
      "gingival_index              float64\n",
      "pocket_depth                float64\n",
      "bleeding_on_probing         float64\n",
      "number_of_missing_teeth       int64\n",
      "mean_mir146a                float64\n",
      "mean_mir146b                float64\n",
      "mean_mir155                 float64\n",
      "mean_mir203                 float64\n",
      "mean_mir223                 float64\n",
      "mean_mir381p                float64\n",
      "mean_GAPDH                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# 1. Load the raw data\n",
    "\n",
    "\n",
    "data_path = (\n",
    "    \"data/raw/Kasim2024-son-veri.csv\"  # Correct path based on repository structure\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "# 2. Check for missing data\n",
    "\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "\n",
    "print(f\"Missing data per column:\\n{missing_data}\")  # Report findings\n",
    "\n",
    "\n",
    "# Handle missing data (if any). Examples (choose one if needed):\n",
    "\n",
    "\n",
    "if missing_data.sum() > 0:\n",
    "\n",
    "    # Mean imputation:\n",
    "\n",
    "    # df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Median imputation:\n",
    "\n",
    "    # df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    print(\"Missing values imputed using [chosen method].\")  # Explain chosen method\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "\n",
    "# 3. Convert 'SEX' and 'GROUP'\n",
    "\n",
    "\n",
    "df[\"SEX\"] = df[\"SEX\"].map({\"F\": 0, \"M\": 1}).astype(\"category\")  # Female: 0, Male: 1\n",
    "\n",
    "\n",
    "df[\"GROUP\"] = df[\"GROUP\"].astype(\"category\")\n",
    "\n",
    "\n",
    "print(f\"Data types after conversion:\\n{df.dtypes}\")\n",
    "\n",
    "\n",
    "# 4. Robust scaling normalization (on the entire dataset initially)\n",
    "\n",
    "\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "df[targets] = scaler.fit_transform(df[targets])\n",
    "\n",
    "\n",
    "# 5. Split data into training and testing sets (after normalization)\n",
    "\n",
    "\n",
    "X = df[\n",
    "    targets\n",
    "    + [\n",
    "        \"SEX\",\n",
    "        \"AGE\",\n",
    "        \"plaque_index\",\n",
    "        \"gingival_index\",\n",
    "        \"pocket_depth\",\n",
    "        \"bleeding_on_probing\",\n",
    "        \"number_of_missing_teeth\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "y = df[\"GROUP\"]\n",
    "\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")  # Corrected to stratify by your group label\n",
    "\n",
    "\n",
    "y_train_numeric = y_train.cat.codes\n",
    "\n",
    "\n",
    "y_test_numeric = y_test.cat.codes\n",
    "\n",
    "\n",
    "# Scaling the training data and saving scaling parameters for testing set\n",
    "\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "\n",
    "# Re-initializing the scaler to ensure it's not contaminated by test data\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "X_train_scaled[targets] = scaler.fit_transform(X_train[targets])\n",
    "\n",
    "\n",
    "X_test_scaled[targets] = scaler.transform(X_test[targets])  # Apply to test data\n",
    "\n",
    "\n",
    "# Ensure the 'src' directory exists\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Saving the scaling parameters so that if we want to apply the same procedure to unseen data, we can load the scaler and transform the data directly using this pickle file.\n",
    "\n",
    "\n",
    "with open(\"src/scaler.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "# Create directories if they don't exist\n",
    "\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "\n",
    "\n",
    "# Save preprocessed data (both raw and scaled training/test sets)\n",
    "\n",
    "\n",
    "df.to_csv(\"data/processed/normalized_mirna_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Save raw and scaled train and test datasets into their respective directories.\n",
    "\n",
    "\n",
    "X_train.to_csv(\"data/processed/X_train_raw.csv\", index=False)\n",
    "\n",
    "\n",
    "X_test.to_csv(\"data/processed/X_test_raw.csv\", index=False)\n",
    "\n",
    "\n",
    "y_train.to_csv(\"data/processed/y_train.csv\", index=False)\n",
    "\n",
    "\n",
    "y_test.to_csv(\"data/processed/y_test.csv\", index=False)\n",
    "\n",
    "\n",
    "X_train_scaled.to_csv(\"data/processed/X_train_scaled.csv\", index=False)\n",
    "\n",
    "\n",
    "X_test_scaled.to_csv(\"data/processed/X_test_scaled.csv\", index=False)\n",
    "\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "\n",
    "X_train.describe().to_csv(\"results/main/descriptive_stats_raw_train.csv\")\n",
    "\n",
    "\n",
    "X_test.describe().to_csv(\"results/main/descriptive_stats_raw_test.csv\")\n",
    "\n",
    "\n",
    "X_train_scaled.describe().to_csv(\"results/main/descriptive_stats_scaled_train.csv\")\n",
    "\n",
    "\n",
    "X_test_scaled.describe().to_csv(\"results/main/descriptive_stats_scaled_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results for GAPDH across groups: F-value = 190.1558, P-value = 0.0000\n",
      "Levene's test results: Statistic = 4.5712, P-value = 0.0125\n",
      "Tukey HSD post hoc test results for GAPDH:\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "     G      P  -4.5173    0.0 -5.1983 -3.8363   True\n",
      "     G      S    0.587 0.1056 -0.0939   1.268  False\n",
      "     P      S   5.1044    0.0  4.4234  5.7853   True\n",
      "----------------------------------------------------\n",
      "Pearson correlation between GAPDH and plaque_index: -0.6614, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and gingival_index: -0.6050, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and pocket_depth: -0.8144, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and bleeding_on_probing: -0.7548, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and number_of_missing_teeth: -0.2378, P-value = 0.0132\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "\n",
    "\n",
    "data_path = r\"data/raw/Kasim2024-son-veri.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "# Check if 'GROUP' column exists\n",
    "if \"GROUP\" not in df.columns:\n",
    "    raise KeyError(\"'GROUP' column is missing from the DataFrame\")\n",
    "\n",
    "# 1. ANOVA and post hoc tests on GAPDH\n",
    "groups = df[\"GROUP\"].unique()\n",
    "\n",
    "\n",
    "gapdh_data = [df[\"mean_GAPDH\"][df[\"GROUP\"] == group] for group in groups]\n",
    "\n",
    "\n",
    "fvalue, pvalue = stats.f_oneway(*gapdh_data)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"ANOVA results for GAPDH across groups: F-value = {fvalue:.4f}, P-value = {pvalue:.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Perform Levene's test for homogeneity of variances\n",
    "levene_stat, levene_p = stats.levene(*gapdh_data)\n",
    "\n",
    "\n",
    "print(f\"Levene's test results: Statistic = {levene_stat:.4f}, P-value = {levene_p:.4f}\")\n",
    "\n",
    "\n",
    "# Perform post hoc Tukey HSD test if ANOVA is significant\n",
    "if pvalue < 0.05:\n",
    "\n",
    "    tukey_result = pairwise_tukeyhsd(df[\"mean_GAPDH\"], df[\"GROUP\"], alpha=0.05)\n",
    "\n",
    "    print(\"Tukey HSD post hoc test results for GAPDH:\")\n",
    "\n",
    "    print(tukey_result)\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"ANOVA not significant; no post hoc test performed.\")\n",
    "\n",
    "\n",
    "# Calculate Pearson correlations between GAPDH Ct and clinical parameters\n",
    "\n",
    "\n",
    "clinical_parameters = [\n",
    "    \"plaque_index\",\n",
    "    \"gingival_index\",\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "    \"number_of_missing_teeth\",\n",
    "]\n",
    "\n",
    "\n",
    "correlations = {}\n",
    "for parameter in clinical_parameters:\n",
    "    correlation, p_value = stats.pearsonr(df[\"mean_GAPDH\"], df[parameter])\n",
    "\n",
    "    correlations[parameter] = {\"correlation\": correlation, \"p_value\": p_value}\n",
    "\n",
    "    print(\n",
    "        f\"Pearson correlation between GAPDH and {parameter}: {correlation:.4f}, P-value = {p_value:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Save results and justification for raw Ct use\n",
    "\n",
    "\n",
    "gapdh_results = pd.DataFrame({\"ANOVA_F\": [fvalue], \"ANOVA_p\": [pvalue]})\n",
    "\n",
    "\n",
    "gapdh_results.to_csv(\"results/supplementary/gapdh_analysis_results.csv\", index=False)\n",
    "correlations_df = pd.DataFrame(correlations).T\n",
    "\n",
    "\n",
    "correlations_df.to_csv(\"results/supplementary/gapdh_correlations.csv\", index=True)\n",
    "\n",
    "\n",
    "justification_text = f\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Justification for Raw Ct Analysis:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Due to the observed significant variability of GAPDH expression across the study groups (ANOVA p-value: {pvalue:.3f}, Tukey HSD results showing significant intergroup differences and effect sizes, Supplementary Table S2), and its correlations with disease severity metrics (Supplementary Figure S1), GAPDH was deemed unsuitable as a reference gene for normalization. Using an unstable reference gene would introduce bias into the analysis, potentially confounding the observed differences in miRNA expression. Therefore, subsequent analyses were performed using raw Ct values. Raw Ct analysis provides a more transparent and unbiased approach when a reliable reference gene cannot be identified. This decision aligns with previous research highlighting the potential for reference gene instability in similar contexts, especially inflammatory conditions such as periodontitis (Dheda et al., 2004; Schmittgen and Zakrajsek, 2000; Li et al., 2019; Peng et al., 2012; Ye et al., 2018). While raw Ct analysis relies on the assumption of similar starting RNA amounts across samples, this limitation is mitigated by our quality control measures during sample processing and robust statistical analyses using non-parametric methods. The variability of GAPDH, as evidenced by its significant correlation with bleeding on probing (Supplementary Figure S1 and Supplementary Table S2), further supports the decision to not use it for normalization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Raw Ct analysis offers several advantages in this context. First, it avoids introducing additional bias or assumptions associated with alternative normalization methods. Second, it focuses on identifying miRNAs exhibiting relatively *large* fold-change differences between groups, where smaller variations introduced by a potentially unstable reference gene are less likely to significantly impact the overall conclusions. Third, the use of robust statistical methods, namely ANOVA followed by post hoc tests with Benjamini-Hochberg FDR correction for multiple comparisons and effect size calculations (Cohen's d), ensures rigorous comparisons of raw Ct values between groups. Despite its limitations, the transparency of raw Ct analysis combined with our rigorous statistical approach and the specific context of our study, makes it a suitable and reliable approach for identifying candidate miRNA biomarkers with altered expression in periodontal disease. If suitable alternative reference genes or normalization methods had been identified, these would have been used instead of the raw Ct approach.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "with open(\"results/supplementary/raw_ct_justification.txt\", \"w\") as f:\n",
    "\n",
    "    f.write(justification_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants for file paths\n",
    "DATA_PATH = \"data/processed\"\n",
    "RESULTS_PATH = \"results/supplementary\"\n",
    "TARGET_MIRNAS = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess the scaled training data by adding the 'GROUP' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled training data with the 'GROUP' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load scaled training data\n",
    "        df_scaled = pd.read_csv(os.path.join(DATA_PATH, \"X_train_scaled.csv\"))\n",
    "        y_train = pd.read_csv(os.path.join(DATA_PATH, \"y_train.csv\"))\n",
    "\n",
    "        # Ensure 'GROUP' column exists in y_train\n",
    "        if \"GROUP\" not in y_train.columns:\n",
    "            raise KeyError(\"The 'GROUP' column is missing from y_train.csv\")\n",
    "\n",
    "        # Add 'GROUP' column to df_scaled\n",
    "        df_scaled[\"GROUP\"] = y_train[\"GROUP\"].values\n",
    "\n",
    "        return df_scaled\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def perform_pca(df, targets):\n",
    "    \"\"\"\n",
    "    Perform PCA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        PCA: The fitted PCA object.\n",
    "        np.ndarray: The PCA-transformed data.\n",
    "        pd.Series: Explained variance ratio for each principal component.\n",
    "    \"\"\"\n",
    "    pca = PCA()\n",
    "    pca_result = pca.fit_transform(df[targets])\n",
    "    explained_variance_pca = pd.Series(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Save explained variance\n",
    "    save_explained_variance(explained_variance_pca, \"pca_explained_variance.csv\")\n",
    "\n",
    "    # Plot scatter plots for each pair of principal components\n",
    "    plot_scatter_pca(pca_result, df[\"GROUP\"], explained_variance_pca)\n",
    "\n",
    "    return pca, pca_result, explained_variance_pca\n",
    "\n",
    "\n",
    "def plot_scatter_pca(pca_result, groups, explained_variance):\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of PCA components.\n",
    "\n",
    "    Args:\n",
    "        pca_result (np.ndarray): PCA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explained_variance (pd.Series): Explained variance ratio for PCA components.\n",
    "    \"\"\"\n",
    "    n_components = pca_result.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=pca_result[:, i],\n",
    "                y=pca_result[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1} ({explained_variance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Principal Component {j+1} ({explained_variance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f\"pca_scatter_pc{i+1}_vs_pc{j+1}.png\"\n",
    "            filepath = os.path.join(RESULTS_PATH, filename)\n",
    "\n",
    "            # Save the plot\n",
    "            save_plot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def perform_lda(df, targets):\n",
    "    \"\"\"\n",
    "    Perform LDA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        LDA: The fitted LDA object.\n",
    "        np.ndarray: The LDA-transformed data.\n",
    "        pd.Series: Explained variance ratio for each linear discriminant.\n",
    "    \"\"\"\n",
    "    lda = LDA()\n",
    "    lda_result = lda.fit_transform(df[targets], df[\"GROUP\"])\n",
    "    explained_variance_lda = pd.Series(lda.explained_variance_ratio_)\n",
    "\n",
    "    # Save explained variance\n",
    "    save_explained_variance(explained_variance_lda, \"lda_explained_variance.csv\")\n",
    "\n",
    "    # Plot scatter plots for each pair of linear discriminants\n",
    "    plot_scatter_lda(lda_result, df[\"GROUP\"], explained_variance_lda)\n",
    "\n",
    "    return lda, lda_result, explained_variance_lda\n",
    "\n",
    "\n",
    "def plot_scatter_lda(lda_result, groups, explained_variance):\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of LDA components.\n",
    "\n",
    "    Args:\n",
    "        lda_result (np.ndarray): LDA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explained_variance (pd.Series): Explained variance ratio for LDA components.\n",
    "    \"\"\"\n",
    "    n_components = lda_result.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=lda_result[:, i],\n",
    "                y=lda_result[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "            plt.xlabel(f\"Linear Discriminant {i+1} ({explained_variance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Linear Discriminant {j+1} ({explained_variance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f\"lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "            filepath = os.path.join(RESULTS_PATH, filename)\n",
    "\n",
    "            # Save the plot\n",
    "            save_plot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def save_explained_variance(explained_variance, filename):\n",
    "    \"\"\"\n",
    "    Save the explained variance ratios to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        explained_variance (pd.Series): Explained variance ratios.\n",
    "        filename (str): The filename for the CSV.\n",
    "    \"\"\"\n",
    "    explained_variance_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Component\": [\n",
    "                f\"PC{i+1}\" if \"pca\" in filename.lower() else f\"LD{i+1}\"\n",
    "                for i in range(len(explained_variance))\n",
    "            ],\n",
    "            \"Explained Variance Ratio\": explained_variance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    filepath = os.path.join(RESULTS_PATH, filename)\n",
    "    explained_variance_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def save_plot(filepath):\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to the specified filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save plot {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to perform PCA and LDA analyses.\n",
    "    \"\"\"\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    df_scaled = load_data()\n",
    "\n",
    "    # Perform PCA analysis\n",
    "    pca, pca_result, explained_variance_pca = perform_pca(df_scaled, TARGET_MIRNAS)\n",
    "\n",
    "    # Perform LDA analysis\n",
    "    lda, lda_result, explained_variance_lda = perform_lda(df_scaled, TARGET_MIRNAS)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (both raw and scaled training sets)\n",
    "df_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "df_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "\n",
    "# Define target miRNAs and clinical parameters\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "clinical_params = [\n",
    "    \"AGE\",\n",
    "    \"plaque_index\",\n",
    "    \"gingival_index\",\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "    \"number_of_missing_teeth\",\n",
    "]\n",
    "\n",
    "# 1. Correlation analysis (Raw Data)\n",
    "correlation_matrix_raw = df_raw[targets + clinical_params].corr(method=\"pearson\")\n",
    "\n",
    "# Create and save correlation heatmap (raw)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix_raw, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Raw Ct Values)\")\n",
    "plt.savefig(\n",
    "    \"results/supplementary/correlation_heatmap_raw.png\"\n",
    ")  # Save to supplementary\n",
    "plt.close()\n",
    "\n",
    "# 2. Correlation analysis (Scaled Data)\n",
    "correlation_matrix_scaled = df_scaled[targets + clinical_params].corr(method=\"pearson\")\n",
    "\n",
    "# Create and save correlation heatmap (scaled)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix_scaled, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Robustly Scaled Ct Values)\")\n",
    "plt.savefig(\"results/main/correlation_heatmap_scaled.png\")  # Save to main results\n",
    "plt.close()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "os.makedirs(\"results/supplementary\", exist_ok=True)\n",
    "\n",
    "# Save correlation matrices\n",
    "correlation_matrix_raw.to_csv(\n",
    "    \"results/supplementary/correlation_matrix_raw.csv\", index=True\n",
    ")\n",
    "correlation_matrix_scaled.to_csv(\n",
    "    \"results/main/correlation_matrix_scaled.csv\", index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ROC analysis...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Filter datasets to include only 'S' and 'P' classes\n",
    "train_filter = y_train[\"GROUP\"].isin([\"S\", \"P\"])\n",
    "test_filter = y_test[\"GROUP\"].isin([\"S\", \"P\"])\n",
    "\n",
    "X_train_raw_cp = X_train_raw[train_filter]\n",
    "X_test_raw_cp = X_test_raw[test_filter]\n",
    "X_train_scaled_cp = X_train_scaled[train_filter]\n",
    "X_test_scaled_cp = X_test_scaled[test_filter]\n",
    "\n",
    "y_train_cp = y_train[train_filter][\"GROUP\"].map({\"S\": 0, \"P\": 1}).reset_index(drop=True)\n",
    "y_test_cp = y_test[test_filter][\"GROUP\"].map({\"S\": 0, \"P\": 1}).reset_index(drop=True)\n",
    "\n",
    "# Define target miRNAs\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "# ROC analysis function\n",
    "def roc_analysis(\n",
    "    X_train, X_test, y_train, y_test, target_name, comparison_name, data_type\n",
    "):\n",
    "    model = LogisticRegression(solver=\"liblinear\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Youden index for optimal cutoff\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    sensitivity = tpr[optimal_idx]\n",
    "    specificity = 1 - fpr[optimal_idx]\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean((y_prob >= optimal_threshold) == y_test)\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"{target_name} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve: {comparison_name} ({data_type})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\n",
    "        f'results/{data_type}/{comparison_name.lower().replace(\" \", \"_\")}',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    plt.savefig(\n",
    "        f'results/{data_type}/{comparison_name.lower().replace(\" \", \"_\")}/roc_curve_{target_name}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"AUC\": roc_auc,\n",
    "        \"Optimal Cutoff\": optimal_threshold,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"PR AUC\": pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "# Perform ROC analysis\n",
    "roc_results_raw_cp = {}\n",
    "roc_results_scaled_cp = {}\n",
    "print(\"Starting ROC analysis...\")\n",
    "for target in targets:\n",
    "    # Select the target feature\n",
    "    X_train_target_raw = X_train_raw_cp[[target]].reset_index(drop=True)\n",
    "    X_test_target_raw = X_test_raw_cp[[target]].reset_index(drop=True)\n",
    "\n",
    "    X_train_target_scaled = X_train_scaled_cp[[target]].reset_index(drop=True)\n",
    "    X_test_target_scaled = X_test_scaled_cp[[target]].reset_index(drop=True)\n",
    "\n",
    "    # Raw data\n",
    "    roc_results_raw_cp[target] = roc_analysis(\n",
    "        X_train_target_raw,\n",
    "        X_test_target_raw,\n",
    "        y_train_cp,\n",
    "        y_test_cp,\n",
    "        target,\n",
    "        \"Control vs. Periodontitis\",\n",
    "        \"raw\",\n",
    "    )\n",
    "\n",
    "    # Scaled data\n",
    "    roc_results_scaled_cp[target] = roc_analysis(\n",
    "        X_train_target_scaled,\n",
    "        X_test_target_scaled,\n",
    "        y_train_cp,\n",
    "        y_test_cp,\n",
    "        target,\n",
    "        \"Control vs. Periodontitis\",\n",
    "        \"scaled\",\n",
    "    )\n",
    "\n",
    "# Save ROC results\n",
    "pd.DataFrame(roc_results_raw_cp).transpose().to_csv(\n",
    "    \"results/supplementary/roc_results_raw_cp.csv\", index=True\n",
    ")\n",
    "pd.DataFrame(roc_results_scaled_cp).transpose().to_csv(\n",
    "    \"results/main/roc_results_scaled_cp.csv\", index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (Previous code for ROC analysis - Part 1) ...\n",
    "\n",
    "# 2 & 3. ROC with Top miRNAs (All Comparisons) and All miRNAs (All Comparisons)\n",
    "\n",
    "# ROC analysis function (same as before, no changes needed)\n",
    "\n",
    "# Prepare data for all pairwise comparisons (raw and scaled)\n",
    "roc_results_raw = {}\n",
    "roc_results_scaled = {}\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][\n",
    "            targets\n",
    "        ].values\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            targets\n",
    "        ].values\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            targets\n",
    "        ].values\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][targets].values\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        roc_results_raw[comparison_name] = {}\n",
    "        roc_results_scaled[comparison_name] = {}\n",
    "\n",
    "        for k, target in enumerate(targets):\n",
    "            # For correct ROC analysis with train and test datasets\n",
    "            roc_results_raw[comparison_name][target] = roc_analysis(\n",
    "                X_train_raw_subset[:, k].reshape(-1, 1),\n",
    "                X_test_raw_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"raw\",\n",
    "            )\n",
    "            roc_results_scaled[comparison_name][target] = roc_analysis(\n",
    "                X_train_scaled_subset[:, k].reshape(-1, 1),\n",
    "                X_test_scaled_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"scaled\",\n",
    "            )\n",
    "\n",
    "# Top miRNAs ROC analysis (all comparisons)\n",
    "top_mirnas = [\"mean_mir146b\", \"mean_mir155\", \"mean_mir203\"]\n",
    "roc_results_top_mirnas_raw = {}\n",
    "roc_results_top_mirnas_scaled = {}\n",
    "\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][top_mirnas].values\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        roc_results_top_mirnas_raw[comparison_name] = {}\n",
    "        roc_results_top_mirnas_scaled[comparison_name] = {}\n",
    "\n",
    "        for k, target in enumerate(top_mirnas):\n",
    "            roc_results_top_mirnas_raw[comparison_name][target] = roc_analysis(\n",
    "                X_train_raw_subset[:, k].reshape(-1, 1),\n",
    "                X_test_raw_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"raw\",\n",
    "            )  # added data_type\n",
    "            roc_results_scaled[comparison_name][target] = roc_analysis(\n",
    "                X_train_scaled_subset[:, k].reshape(-1, 1),\n",
    "                X_test_scaled_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"scaled\",\n",
    "            )  # added data_type\n",
    "\n",
    "# Save ROC results\n",
    "# ... (saving of ROC results will be done after combining miRNAs for ROC is calculated, for organization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (previous ROC analysis code) ...\n",
    "\n",
    "# Combining Top miRNAs for ROC Analysis\n",
    "\n",
    "roc_results_combined_raw = {}\n",
    "roc_results_combined_scaled = {}\n",
    "roc_results_combined_raw_then_scaled = {}  # For raw combined, then scaled\n",
    "roc_results_combined_scaled_then_combined = {}  # For scaled combined, then combined\n",
    "\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][top_mirnas]\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            top_mirnas\n",
    "        ]\n",
    "\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ]\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][top_mirnas]\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Combine raw Ct values, then scale\n",
    "        X_train_raw_combined = X_train_raw_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "        X_test_raw_combined = X_test_raw_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "        X_train_raw_combined_scaled = scaler.fit_transform(X_train_raw_combined)\n",
    "        X_test_raw_combined_scaled = scaler.transform(X_test_raw_combined)\n",
    "\n",
    "        # Combine scaled Ct values\n",
    "        X_train_scaled_combined = X_train_scaled_subset.mean(axis=1).values.reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        X_test_scaled_combined = X_test_scaled_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "        roc_results_combined_raw[comparison_name] = roc_analysis(\n",
    "            X_train_raw_combined,\n",
    "            X_test_raw_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_raw\",\n",
    "            comparison_name,\n",
    "            \"raw\",\n",
    "        )\n",
    "        roc_results_combined_scaled[comparison_name] = roc_analysis(\n",
    "            X_train_scaled_combined,\n",
    "            X_test_scaled_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_scaled\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "        roc_results_combined_raw_then_scaled[comparison_name] = roc_analysis(\n",
    "            X_train_raw_combined_scaled,\n",
    "            X_test_raw_combined_scaled,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_raw_scaled\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "        roc_results_combined_scaled_then_combined[comparison_name] = roc_analysis(\n",
    "            X_train_scaled_combined,\n",
    "            X_test_scaled_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_scaled_avg\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "\n",
    "# Saving ROC Results (all combined in a dictionary)\n",
    "roc_results_all = {\n",
    "    \"raw\": roc_results_raw,\n",
    "    \"scaled\": roc_results_scaled,\n",
    "    \"combined_raw\": roc_results_combined_raw,\n",
    "    \"combined_scaled\": roc_results_combined_scaled,\n",
    "    \"top_mirnas_raw\": roc_results_top_mirnas_raw,\n",
    "    \"top_mirnas_scaled\": roc_results_top_mirnas_scaled,\n",
    "    \"combined_raw_then_scaled\": roc_results_combined_raw_then_scaled,\n",
    "    \"combined_scaled_then_combined\": roc_results_combined_scaled_then_combined,\n",
    "}\n",
    "\n",
    "# Iterate over each data type and create tables\n",
    "for data_type, results in roc_results_all.items():\n",
    "    for comparison, values in results.items():\n",
    "        # Determine the output directory based on data type\n",
    "        output_subdir = (\n",
    "            \"main\"\n",
    "            if \"scaled\" in data_type or \"combined\" in data_type\n",
    "            else \"supplementary\"\n",
    "        )\n",
    "        output_dir = f\"results/{output_subdir}/roc_{comparison.lower().replace(' ', '_')}_{data_type}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Wrap 'values' in a list to create a DataFrame\n",
    "        temp = pd.DataFrame([values])\n",
    "        temp[\"Comparison\"] = comparison\n",
    "        temp[\"DataType\"] = data_type\n",
    "\n",
    "        # Save the DataFrame to CSV\n",
    "        temp.to_csv(\n",
    "            f\"{output_dir}/roc_metrics_{data_type}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "import json\n",
    "\n",
    "# Save the entire dictionary to a JSON file\n",
    "with open(\"results/main/roc_results_all.json\", \"w\") as fp:\n",
    "    json.dump(roc_results_all, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants for file paths\n",
    "DATA_PATH = \"data/processed\"\n",
    "RESULTS_PATH = \"results/supplementary\"\n",
    "TARGET_MIRNAS = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "def loadData() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess the scaled training data by adding the 'GROUP' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled training data with the 'GROUP' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load scaled training data\n",
    "        dfScaled = pd.read_csv(os.path.join(DATA_PATH, \"X_train_scaled.csv\"))\n",
    "        yTrain = pd.read_csv(os.path.join(DATA_PATH, \"y_train.csv\"))\n",
    "\n",
    "        # Ensure 'GROUP' column exists in y_train\n",
    "        if \"GROUP\" not in yTrain.columns:\n",
    "            raise KeyError(\"The 'GROUP' column is missing from y_train.csv\")\n",
    "\n",
    "        # Add 'GROUP' column to dfScaled\n",
    "        dfScaled[\"GROUP\"] = yTrain[\"GROUP\"].values\n",
    "\n",
    "        return dfScaled\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def performPCA(df: pd.DataFrame, targets: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform PCA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        PCA: The fitted PCA object.\n",
    "        np.ndarray: The PCA-transformed data.\n",
    "        pd.Series: Explained variance ratio for each principal component.\n",
    "    \"\"\"\n",
    "    pca = PCA()\n",
    "    pcaResult = pca.fit_transform(df[targets])\n",
    "    explainedVariancePCA = pd.Series(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Save explained variance\n",
    "    saveExplainedVariance(explainedVariancePCA, \"pca_explained_variance.csv\")\n",
    "\n",
    "    # Plot scatter plots for each pair of principal components\n",
    "    plotScatterPCA(pcaResult, df[\"GROUP\"], explainedVariancePCA)\n",
    "\n",
    "    return pca, pcaResult, explainedVariancePCA\n",
    "\n",
    "\n",
    "def plotScatterPCA(\n",
    "    pcaResult: np.ndarray, groups: pd.Series, explainedVariance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of PCA components.\n",
    "\n",
    "    Args:\n",
    "        pcaResult (np.ndarray): PCA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explainedVariance (pd.Series): Explained variance ratio for PCA components.\n",
    "    \"\"\"\n",
    "    nComponents = pcaResult.shape[1]\n",
    "    for i in range(nComponents):\n",
    "        for j in range(i + 1, nComponents):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=pcaResult[:, i],\n",
    "                y=pcaResult[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1} ({explainedVariance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Principal Component {j+1} ({explainedVariance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f\"pca_scatter_pc{i+1}_vs_pc{j+1}.png\"\n",
    "            filepath = os.path.join(RESULTS_PATH, filename)\n",
    "\n",
    "            # Save the plot\n",
    "            savePlot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def performLDA(df: pd.DataFrame, targets: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Perform LDA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        LDA: The fitted LDA object.\n",
    "        np.ndarray: The LDA-transformed data.\n",
    "        pd.Series: Explained variance ratio for each linear discriminant.\n",
    "    \"\"\"\n",
    "    lda = LDA()\n",
    "    ldaResult = lda.fit_transform(df[targets], df[\"GROUP\"])\n",
    "    explainedVarianceLDA = pd.Series(lda.explained_variance_ratio_)\n",
    "\n",
    "    # Save explained variance\n",
    "    saveExplainedVariance(explainedVarianceLDA, \"lda_explained_variance.csv\")\n",
    "\n",
    "    # Plot scatter plots for each pair of linear discriminants\n",
    "    plotScatterLDA(ldaResult, df[\"GROUP\"], explainedVarianceLDA)\n",
    "\n",
    "    return lda, ldaResult, explainedVarianceLDA\n",
    "\n",
    "\n",
    "def plotScatterLDA(\n",
    "    ldaResult: np.ndarray, groups: pd.Series, explainedVariance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of LDA components.\n",
    "\n",
    "    Args:\n",
    "        ldaResult (np.ndarray): LDA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explainedVariance (pd.Series): Explained variance ratio for LDA components.\n",
    "    \"\"\"\n",
    "    nComponents = ldaResult.shape[1]\n",
    "    for i in range(nComponents):\n",
    "        for j in range(i + 1, nComponents):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=ldaResult[:, i],\n",
    "                y=ldaResult[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "            plt.xlabel(f\"Linear Discriminant {i+1} ({explainedVariance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Linear Discriminant {j+1} ({explainedVariance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename = f\"lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "            filepath = os.path.join(RESULTS_PATH, filename)\n",
    "\n",
    "            # Save the plot\n",
    "            savePlot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def saveExplainedVariance(explainedVariance: pd.Series, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the explained variance ratios to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        explainedVariance (pd.Series): Explained variance ratios.\n",
    "        filename (str): The filename for the CSV.\n",
    "    \"\"\"\n",
    "    explainedVarianceDf = pd.DataFrame(\n",
    "        {\n",
    "            \"Component\": [\n",
    "                f\"PC{i+1}\" if \"pca\" in filename.lower() else f\"LD{i+1}\"\n",
    "                for i in range(len(explainedVariance))\n",
    "            ],\n",
    "            \"Explained Variance Ratio\": explainedVariance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    filepath = os.path.join(RESULTS_PATH, filename)\n",
    "    explainedVarianceDf.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def savePlot(filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to the specified filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save plot {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to perform PCA and LDA analyses.\n",
    "    \"\"\"\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "    # Load and preprocess data\n",
    "    dfScaled = loadData()\n",
    "\n",
    "    # Perform PCA analysis\n",
    "    pca, pcaResult, explainedVariancePCA = performPCA(dfScaled, TARGET_MIRNAS)\n",
    "\n",
    "    # Perform LDA analysis\n",
    "    lda, ldaResult, explainedVarianceLDA = performLDA(dfScaled, TARGET_MIRNAS)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "# Suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "# Load preprocessed data (scaled training and test sets)\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert GROUP column to numerical using cat.codes\n",
    "y_train_numeric = y_train[\"GROUP\"].astype(\"category\").cat.codes\n",
    "y_test_numeric = y_test[\"GROUP\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Define target miRNAs (features)\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "X_train = X_train_scaled[targets]\n",
    "X_test = X_test_scaled[targets]\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000, solver=\"liblinear\"\n",
    "    ),  # Increased max_iter and added solver for better convergence\n",
    "    \"LDA\": LDA(),\n",
    "    \"SVM\": SVC(\n",
    "        probability=True\n",
    "    ),  # Add probability parameter so that roc_auc_score can be calculated\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Neural Network\": MLPClassifier(\n",
    "        max_iter=1000, random_state=42, early_stopping=True\n",
    "    ),  # Increased max_iter and add early stopping to avoid overfitting\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train_numeric)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = (\n",
    "        clf.predict_proba(X_test)\n",
    "        if hasattr(clf, \"predict_proba\")\n",
    "        else clf.decision_function(X_test)\n",
    "    )  # Get predicted probabilities or decision function if probabilities are not available\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test_numeric, y_pred),\n",
    "        \"precision\": precision_score(\n",
    "            y_test_numeric, y_pred, average=\"weighted\", zero_division=0\n",
    "        ),\n",
    "        \"recall\": recall_score(\n",
    "            y_test_numeric, y_pred, average=\"weighted\", zero_division=0\n",
    "        ),\n",
    "        \"f1_score\": f1_score(\n",
    "            y_test_numeric, y_pred, average=\"weighted\", zero_division=0\n",
    "        ),\n",
    "    }\n",
    "    # Compute roc_auc_score only for binary and multiclass problems\n",
    "    if len(set(y_test_numeric)) <= 2:\n",
    "        try:\n",
    "            results[name][\"auc\"] = roc_auc_score(y_test_numeric, y_prob)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"Only one class present in y_true for {name}. ROC AUC score is not defined. Skipping AUC calculation.\"\n",
    "            )\n",
    "    else:\n",
    "        results[name][\"auc\"] = roc_auc_score(y_test_numeric, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "    # Add classification report to show precision, recall and f1-score for each class\n",
    "    results[name][\"classification_report\"] = classification_report(\n",
    "        y_test_numeric, y_pred, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Add confusion matrix as well\n",
    "    cm = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\"g\",\n",
    "        xticklabels=sorted(y_train[\"GROUP\"].unique()),\n",
    "        yticklabels=sorted(y_train[\"GROUP\"].unique()),\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.savefig(f\"results/main/{name}_cm.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importance = classifiers[\"Random Forest\"].feature_importances_\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": targets, \"Importance\": feature_importance}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "\n",
    "# Convert the dictionaries to DataFrames and save the results\n",
    "pd.DataFrame(results).transpose().to_csv(\n",
    "    \"results/main/classification_results.csv\", index=True\n",
    ")\n",
    "\n",
    "feature_importance_df.to_csv(\"results/main/feature_importance.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Corrected Code for miRNA Analysis with ROC Evaluation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Suppress UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load preprocessed data from CSV files.\n",
    "\n",
    "    Returns:\n",
    "        X_train_raw (pd.DataFrame): Raw training features.\n",
    "        X_test_raw (pd.DataFrame): Raw test features.\n",
    "        X_train_scaled (pd.DataFrame): Scaled training features.\n",
    "        X_test_scaled (pd.DataFrame): Scaled test features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Test labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "\n",
    "        X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "\n",
    "        X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "\n",
    "        X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "        y_train = pd.read_csv(\"data/processed/y_train.csv\")[\"GROUP\"]\n",
    "        y_test = pd.read_csv(\"data/processed/y_test.csv\")[\"GROUP\"]\n",
    "\n",
    "        return X_train_raw, X_test_raw, X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def encode_labels(y_train, y_test):\n",
    "    \"\"\"\n",
    "    Encode categorical labels to numerical codes.\n",
    "\n",
    "    Args:\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Test labels.\n",
    "\n",
    "    Returns:\n",
    "        y_train_numeric (pd.Series): Encoded training labels.\n",
    "        y_test_numeric (pd.Series): Encoded test labels.\n",
    "    \"\"\"\n",
    "    if isinstance(y_train.dtype, pd.CategoricalDtype):\n",
    "        y_train_numeric = y_train.cat.codes\n",
    "        y_test_numeric = y_test.cat.codes\n",
    "    else:\n",
    "        y_train_numeric = y_train\n",
    "        y_test_numeric = y_test\n",
    "    return y_train_numeric, y_test_numeric\n",
    "\n",
    "\n",
    "def roc_analysis(\n",
    "    X_train, X_test, y_train, y_test, target_name, comparison_name, data_type\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Perform ROC analysis using Logistic Regression and compute metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (np.ndarray): Training features.\n",
    "    - X_test (np.ndarray): Testing features.\n",
    "    - y_train (np.ndarray): Training labels.\n",
    "    - y_test (np.ndarray): Testing labels.\n",
    "    - target_name (str): Name of the target feature.\n",
    "    - comparison_name (str): Description of the comparison.\n",
    "    - data_type (str): Type of data ('raw' or 'scaled').\n",
    "\n",
    "    Returns:\n",
    "    - dict: ROC metrics including AUC, optimal cutoff, sensitivity, specificity, accuracy, and PR AUC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = LogisticRegression(solver=\"liblinear\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Youden's J statistic for optimal cutoff\n",
    "        j_scores = tpr - fpr\n",
    "        optimal_idx = np.argmax(j_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        sensitivity = tpr[optimal_idx]\n",
    "        specificity = 1 - fpr[optimal_idx]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_pred = (y_prob >= optimal_threshold).astype(int)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Precision-Recall AUC\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        pr_auc = auc(recall, precision)\n",
    "\n",
    "        # Plot ROC Curve\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr, tpr, label=f\"{target_name} (AUC = {roc_auc:.2f})\")\n",
    "        plt.plot([0, 1], [0, 1], \"k--\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(f\"ROC Curve: {comparison_name} ({data_type})\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save ROC Curve\n",
    "        roc_dir = os.path.join(\n",
    "            \"results\", data_type, comparison_name.lower().replace(\" \", \"_\")\n",
    "        )\n",
    "        os.makedirs(roc_dir, exist_ok=True)\n",
    "        roc_path = os.path.join(roc_dir, f\"roc_curve_{target_name}.png\")\n",
    "        plt.savefig(roc_path)\n",
    "        plt.close()\n",
    "\n",
    "        metrics = {\n",
    "            \"AUC\": roc_auc,\n",
    "            \"Optimal Cutoff\": optimal_threshold,\n",
    "            \"Sensitivity\": sensitivity,\n",
    "            \"Specificity\": specificity,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"PR AUC\": pr_auc,\n",
    "        }\n",
    "\n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"ROC analysis failed for {target_name} in {comparison_name} ({data_type}): {e}\"\n",
    "        )\n",
    "        return {}\n",
    "\n",
    "\n",
    "def save_roc_results(roc_results, data_type):\n",
    "    \"\"\"\n",
    "    Save ROC results to CSV files.\n",
    "\n",
    "    Args:\n",
    "        roc_results (dict): Nested dictionary containing ROC metrics.\n",
    "        data_type (str): Type of data ('combined_raw', 'combined_scaled', etc.).\n",
    "    \"\"\"\n",
    "    for comparison, methods_metrics in roc_results.items():\n",
    "        try:\n",
    "            # Convert methods_metrics dict to DataFrame\n",
    "            temp_df = pd.DataFrame.from_dict(\n",
    "                methods_metrics, orient=\"index\"\n",
    "            ).reset_index()\n",
    "\n",
    "            temp_df = temp_df.rename(columns={\"index\": \"method\"})\n",
    "\n",
    "            # Define output directory and ensure it exists\n",
    "            output_dir = os.path.join(\n",
    "                \"results\",\n",
    "                \"main\",\n",
    "                f\"roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "            )\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Save DataFrame to CSV\n",
    "            csv_filename = f\"roc_results_{data_type}.csv\"\n",
    "            csv_path = os.path.join(output_dir, csv_filename)\n",
    "            temp_df.to_csv(csv_path, index=False)\n",
    "        except ValueError as ve:\n",
    "            print(\n",
    "                f\"ValueError while saving ROC results for {comparison}, {data_type}: {ve}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save ROC results for {comparison}, {data_type}: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    X_train_raw, X_test_raw, X_train_scaled, X_test_scaled, y_train, y_test = (\n",
    "        load_data()\n",
    "    )\n",
    "\n",
    "    # Encode labels\n",
    "\n",
    "    y_train_numeric, y_test_numeric = encode_labels(y_train, y_test)\n",
    "\n",
    "    # Define target miRNAs and groups\n",
    "\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "\n",
    "    top_targets = [\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "    ]  # Based on prior feature selection\n",
    "\n",
    "    groups = [\"S\", \"G\", \"P\"]\n",
    "\n",
    "    # 1. Combined miRNA Scores (using training data only for proper assessment)\n",
    "\n",
    "    X_train_combined = X_train_scaled.copy()\n",
    "\n",
    "    X_test_combined = X_test_scaled.copy()\n",
    "\n",
    "    # Simple Average\n",
    "\n",
    "    X_train_combined[\"combined_avg\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "    X_test_combined[\"combined_avg\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "    # Weighted Average (using feature importances from Random Forest)\n",
    "\n",
    "    try:\n",
    "        with open(\"src/scaler.pkl\", \"rb\") as f:\n",
    "\n",
    "            scaler_loaded = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        print(\"Scaler file not found. Proceeding with new RobustScaler.\")\n",
    "        scaler_loaded = RobustScaler()\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    rf.fit(X_train_scaled[targets], y_train_numeric)  # Using y_train_numeric here\n",
    "\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    top_mirna_indices = [targets.index(mirna) for mirna in top_targets]\n",
    "\n",
    "    top_mirna_importances = feature_importances[top_mirna_indices]\n",
    "\n",
    "    X_train_combined[\"combined_weighted\"] = np.dot(\n",
    "        X_train_scaled[top_targets],\n",
    "        top_mirna_importances / np.sum(top_mirna_importances),\n",
    "    )\n",
    "\n",
    "    X_test_combined[\"combined_weighted\"] = np.dot(\n",
    "        X_test_scaled[top_targets],\n",
    "        top_mirna_importances / np.sum(top_mirna_importances),\n",
    "    )\n",
    "\n",
    "    # PCA (First Principal Component)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "\n",
    "    X_train_combined[\"combined_pca\"] = pca.fit_transform(X_train_scaled[top_targets])\n",
    "\n",
    "    X_test_combined[\"combined_pca\"] = pca.transform(X_test_scaled[top_targets])\n",
    "\n",
    "    # LDA (First Linear Discriminant)\n",
    "\n",
    "    lda = LDA(n_components=1)\n",
    "\n",
    "    X_train_combined[\"combined_lda\"] = lda.fit_transform(\n",
    "        X_train_scaled[top_targets], y_train_numeric\n",
    "    )\n",
    "\n",
    "    X_test_combined[\"combined_lda\"] = lda.transform(X_test_scaled[top_targets])\n",
    "\n",
    "    combined_methods = [\n",
    "        \"combined_avg\",\n",
    "        \"combined_weighted\",\n",
    "        \"combined_pca\",\n",
    "        \"combined_lda\",\n",
    "    ]\n",
    "\n",
    "    # 2. ROC Analysis with Combined Scores (Raw and Scaled, All Comparisons)\n",
    "\n",
    "    # Initialize dictionaries to store ROC results\n",
    "\n",
    "    roc_results_combined_raw = {}\n",
    "\n",
    "    roc_results_combined_scaled = {}\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "\n",
    "        for j in range(i + 1, len(groups)):\n",
    "\n",
    "            comparison = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "            # ROC for Averaging Raw Values and Then Scaling\n",
    "\n",
    "            try:\n",
    "                temp_X_raw = X_train_raw[targets].copy()\n",
    "\n",
    "                temp_X_test = X_test_raw[targets].copy()\n",
    "\n",
    "                temp_X_raw[\"combined_top_raw\"] = temp_X_raw[top_targets].mean(axis=1)\n",
    "\n",
    "                temp_X_test[\"combined_top_raw\"] = temp_X_test[top_targets].mean(axis=1)\n",
    "\n",
    "                # Subset the combined data, scale and perform ROC using the scaled data.\n",
    "\n",
    "                train_mask = y_train.isin([groups[i], groups[j]])\n",
    "\n",
    "                test_mask = y_test.isin([groups[i], groups[j]])\n",
    "\n",
    "                combined_X_train_raw = temp_X_raw[train_mask][\n",
    "                    \"combined_top_raw\"\n",
    "                ].values.reshape(-1, 1)\n",
    "\n",
    "                combined_X_test_raw = temp_X_test[test_mask][\n",
    "                    \"combined_top_raw\"\n",
    "                ].values.reshape(-1, 1)\n",
    "\n",
    "                combined_X_train_raw_scaled = scaler_loaded.fit_transform(\n",
    "                    combined_X_train_raw\n",
    "                )\n",
    "\n",
    "                combined_X_test_raw_scaled = scaler_loaded.transform(\n",
    "                    combined_X_test_raw\n",
    "                )\n",
    "\n",
    "                y_train_subset = (\n",
    "                    y_train[train_mask]\n",
    "                    .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "                    .values\n",
    "                )\n",
    "\n",
    "                y_test_subset = (\n",
    "                    y_test[test_mask].apply(lambda x: 0 if x == groups[i] else 1).values\n",
    "                )\n",
    "\n",
    "                # Initialize the result dictionaries for the current comparison.\n",
    "\n",
    "                roc_results_combined_raw[comparison] = {}\n",
    "\n",
    "                roc_results_combined_scaled[comparison] = {}\n",
    "\n",
    "                # ROC for combined_top_raw_scaled\n",
    "                roc_results_combined_raw[comparison][\"combined_top_raw_scaled\"] = (\n",
    "                    roc_analysis(\n",
    "                        combined_X_train_raw_scaled,\n",
    "                        combined_X_test_raw_scaled,\n",
    "                        y_train_subset,\n",
    "                        y_test_subset,\n",
    "                        \"combined_top_raw_scaled\",\n",
    "                        comparison,\n",
    "                        \"scaled\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                for method in combined_methods:\n",
    "\n",
    "                    # Subset data for current comparison\n",
    "\n",
    "                    current_X_train = X_train_combined[train_mask][\n",
    "                        method\n",
    "                    ].values.reshape(-1, 1)\n",
    "\n",
    "                    current_X_test = X_test_combined[test_mask][method].values.reshape(\n",
    "                        -1, 1\n",
    "                    )\n",
    "\n",
    "                    # Perform ROC analysis\n",
    "\n",
    "                    roc_results_combined_scaled[comparison][method] = roc_analysis(\n",
    "                        current_X_train,\n",
    "                        current_X_test,\n",
    "                        y_train_subset,\n",
    "                        y_test_subset,\n",
    "                        method,\n",
    "                        comparison,\n",
    "                        \"scaled\",\n",
    "                    )\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Error during ROC analysis for comparison {comparison}: {e}\")\n",
    "\n",
    "    # Saving ROC Results (all methods together):\n",
    "\n",
    "    roc_results_combined_all = {\n",
    "        \"combined_raw\": roc_results_combined_raw,\n",
    "        \"combined_scaled\": roc_results_combined_scaled,\n",
    "    }\n",
    "\n",
    "    # Iterate over each data type and create tables\n",
    "\n",
    "    for data_type, results in roc_results_combined_all.items():\n",
    "\n",
    "        for comparison, values in results.items():\n",
    "\n",
    "            try:\n",
    "                # Create directory for the ROC results if it doesn't exist\n",
    "\n",
    "                output_dir = os.path.join(\n",
    "                    \"results\",\n",
    "                    \"main\",\n",
    "                    f\"roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "                )\n",
    "\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                # Convert methods_metrics dict to DataFrame\n",
    "                temp_df = pd.DataFrame.from_dict(values, orient=\"index\").reset_index()\n",
    "\n",
    "                temp_df = temp_df.rename(columns={\"index\": \"method\"})\n",
    "\n",
    "                # Save DataFrame to CSV\n",
    "                csv_filename = f\"roc_results_{data_type}.csv\"\n",
    "                csv_path = os.path.join(output_dir, csv_filename)\n",
    "\n",
    "                temp_df.to_csv(csv_path, index=False)\n",
    "            except ValueError as ve:\n",
    "\n",
    "                print(\n",
    "                    f\"ValueError while saving ROC results for {comparison} ({data_type}): {ve}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save ROC results for {comparison} ({data_type}): {e}\")\n",
    "\n",
    "    # Save the entire dictionary to JSON\n",
    "    try:\n",
    "\n",
    "        json_path = os.path.join(\"results\", \"main\", \"roc_results_combined_all.json\")\n",
    "        with open(json_path, \"w\") as fp:\n",
    "\n",
    "            json.dump(roc_results_combined_all, fp, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save ROC results to JSON file: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Centaurioun\\AppData\\Local\\Temp\\ipykernel_9684\\2358905895.py:19: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (scaled training and testing sets)\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "\n",
    "    # Define target miRNAs (features)\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "    X_train = X_train_scaled[targets]\n",
    "    X_test = X_test_scaled[targets]\n",
    "\n",
    "    # 1. PCA (All Components)\n",
    "    pca = PCA()  # Keep all components\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    explained_variance_pca = pca.explained_variance_ratio_\n",
    "\n",
    "    # Pairwise 2D scatter plots of PCA components (on training data)\n",
    "    n_components = X_train_pca.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=X_train_pca[:, i],\n",
    "                y=X_train_pca[:, j],\n",
    "                hue=y_train[\"GROUP\"],\n",
    "                palette=\"viridis\",\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1}\")\n",
    "            plt.ylabel(f\"Principal Component {j+1}\")\n",
    "            plt.savefig(f\"results/supplementary/pca_scatter_pc{i+1}_vs_pc{j+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # 3D scatter plot of the first three components (on training data)\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection=\"3d\")\n",
    "            ax.scatter(\n",
    "                X_train_pca[:, 0],\n",
    "                X_train_pca[:, 1],\n",
    "                X_train_pca[:, 2],\n",
    "                c=y_train[\"GROUP\"].astype(\"category\").cat.codes,\n",
    "                cmap=\"viridis\",\n",
    "            )\n",
    "            ax.set_title(\"3D PCA Scatter Plot (PC1 vs PC2 vs PC3)\")\n",
    "            ax.set_xlabel(\"Principal Component 1\")\n",
    "            ax.set_ylabel(\"Principal Component 2\")\n",
    "            ax.set_zlabel(\"Principal Component 3\")\n",
    "            plt.savefig(\"results/supplementary/pca_scatter_3d.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Save explained variance ratios\n",
    "            explained_variance_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"PC\": [f\"PC{i+1}\" for i in range(len(explained_variance_pca))],\n",
    "                    \"Explained Variance Ratio\": explained_variance_pca,\n",
    "                }\n",
    "            )\n",
    "            # Create directories if they don't exist\n",
    "            os.makedirs(\"results/supplementary\", exist_ok=True)\n",
    "\n",
    "            explained_variance_df.to_csv(\n",
    "                \"results/supplementary/pca_explained_variance.csv\", index=False\n",
    "            )\n",
    "\n",
    "            # 2. LDA (All Components)\n",
    "            lda = LDA()  # Retaining all components\n",
    "            X_train_lda = lda.fit_transform(\n",
    "                X_train, y_train_numeric\n",
    "            )  # Correctly use group labels for LDA\n",
    "            X_test_lda = lda.transform(X_test)\n",
    "            explained_variance_lda = lda.explained_variance_ratio_\n",
    "\n",
    "            # Pairwise 2-D scatter plots for LDA\n",
    "            n_components_lda = X_train_lda.shape[1]\n",
    "            for i in range(n_components_lda):\n",
    "                for j in range(i + 1, n_components_lda):\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    sns.scatterplot(\n",
    "                        x=X_train_lda[:, i],\n",
    "                        y=X_train_lda[:, j],\n",
    "                        hue=y_train[\"GROUP\"],\n",
    "                        palette=\"viridis\",\n",
    "                    )\n",
    "                    plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "                    plt.xlabel(f\"Linear Discriminant {i+1}\")\n",
    "                    plt.ylabel(f\"Linear Discriminant {j+1}\")\n",
    "                    plt.savefig(\n",
    "                        f\"results/supplementary/lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "                    )\n",
    "                    plt.close()\n",
    "\n",
    "                    # Save explained variance ratios\n",
    "                    explained_variance_lda_df = pd.DataFrame(\n",
    "                        {\n",
    "                            \"LD\": [\n",
    "                                f\"LD{i+1}\" for i in range(len(explained_variance_lda))\n",
    "                            ],\n",
    "                            \"Explained Variance Ratio\": explained_variance_lda,\n",
    "                        }\n",
    "                    )\n",
    "                    explained_variance_lda_df.to_csv(\n",
    "                        \"results/supplementary/lda_explained_variance.csv\", index=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Centaurioun\\AppData\\Local\\Temp\\ipykernel_9684\\3039201315.py:31: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load preprocessed data (scaled training and test sets)\n",
    "X_train = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "\n",
    "    # Define target miRNAs (features)\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "\n",
    "    X_train = X_train[targets]\n",
    "    X_test = X_test[targets]\n",
    "\n",
    "    # Initialize classifiers\n",
    "    classifiers = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, solver=\"liblinear\"),\n",
    "        \"LDA\": LDA(),\n",
    "        \"SVM\": SVC(probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "        \"Neural Network\": MLPClassifier(\n",
    "            max_iter=1000, random_state=42, early_stopping=True\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train_numeric)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_prob = (\n",
    "            clf.predict_proba(X_test)\n",
    "            if hasattr(clf, \"predict_proba\")\n",
    "            else clf.decision_function(X_test)\n",
    "        )\n",
    "\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy_score(y_test_numeric, y_pred),\n",
    "            \"precision\": precision_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "            \"recall\": recall_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "            \"f1_score\": f1_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "        }\n",
    "        # Compute roc_auc_score only for binary and multiclass problems\n",
    "        if len(set(y_test_numeric)) <= 2:\n",
    "            try:\n",
    "                results[name][\"auc\"] = roc_auc_score(y_test_numeric, y_prob)\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"Only one class present in y_true. ROC AUC score is not defined in that case. Skipping AUC calculation.\"\n",
    "                )\n",
    "            else:\n",
    "                results[name][\"auc\"] = roc_auc_score(\n",
    "                    y_test_numeric, y_prob, multi_class=\"ovr\"\n",
    "                )\n",
    "\n",
    "            # Add classification report and confusion matrix\n",
    "            results[name][\"classification_report\"] = classification_report(\n",
    "                y_test_numeric, y_pred, output_dict=True, zero_division=0\n",
    "            )\n",
    "            cm = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(\n",
    "                cm,\n",
    "                annot=True,\n",
    "                cmap=\"Blues\",\n",
    "                fmt=\"g\",\n",
    "                xticklabels=sorted(y_test[\"GROUP\"].unique()),\n",
    "                yticklabels=sorted(y_test[\"GROUP\"].unique()),\n",
    "            )  # corrected labels\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.title(f\"Confusion Matrix: {name}\")\n",
    "            plt.savefig(f\"results/main/{name}_cm.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Feature importance for Random Forest\n",
    "            feature_importance = classifiers[\"Random Forest\"].feature_importances_\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": targets, \"Importance\": feature_importance}\n",
    "            ).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "            # Create directories if they don't exist\n",
    "            os.makedirs(\"results/main\", exist_ok=True)\n",
    "\n",
    "            # Convert the dictionaries to DataFrames and save the results\n",
    "            pd.DataFrame(results).transpose().to_csv(\n",
    "                \"results/main/classification_results.csv\", index=True\n",
    "            )\n",
    "            feature_importance_df.to_csv(\n",
    "                \"results/main/feature_importance.csv\", index=False\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Centaurioun\\AppData\\Local\\Temp\\ipykernel_9684\\2735949905.py:25: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load preprocessed data (raw and scaled training/test sets)\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "\n",
    "    # Define target miRNAs and groups\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "    top_targets = [\"mean_mir146b\", \"mean_mir155\", \"mean_mir203\"]\n",
    "    groups = [\"S\", \"G\", \"P\"]\n",
    "\n",
    "    # ROC analysis function (same as before)\n",
    "\n",
    "    def roc_analysis(\n",
    "        X_train, X_test, y_train, y_test, target_name, comparison_name, data_type\n",
    "    ):\n",
    "        model = LogisticRegression(solver=\"liblinear\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        sensitivity = tpr[optimal_idx]\n",
    "        specificity = 1 - fpr[optimal_idx]\n",
    "        try:\n",
    "            accuracy = sum(y_test == (y_prob > optimal_threshold)) / len(y_test)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error calculating accuracy for {target_name} ({comparison_name}, {data_type}): {e}\"\n",
    "            )\n",
    "            accuracy = np.nan\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            return {\n",
    "                \"AUC\": roc_auc,\n",
    "                \"Optimal Cutoff\": optimal_threshold,\n",
    "                \"Sensitivity\": sensitivity,\n",
    "                \"Specificity\": specificity,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"PR AUC\": pr_auc,\n",
    "            }\n",
    "\n",
    "            # 1. Combined miRNA Scores (on training data)\n",
    "            combined_methods = [\n",
    "                \"combined_avg\",\n",
    "                \"combined_weighted\",\n",
    "                \"combined_pca\",\n",
    "                \"combined_lda\",\n",
    "            ]\n",
    "\n",
    "            X_train_combined = X_train_scaled.copy()\n",
    "            X_test_combined = X_test_scaled.copy()\n",
    "\n",
    "            # Simple Average\n",
    "            X_train_combined[\"combined_avg\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "            X_test_combined[\"combined_avg\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "            # Weighted Average (using feature importances from Random Forest)\n",
    "            with open(\"src/scaler.pkl\", \"rb\") as f:\n",
    "                scaler_loaded = pickle.load(f)\n",
    "\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                rf.fit(\n",
    "                    X_train_scaled[targets], y_train_numeric\n",
    "                )  # Fit on numerical labels\n",
    "                feature_importances = rf.feature_importances_\n",
    "                top_mirna_indices = [targets.index(mirna) for mirna in top_targets]\n",
    "                top_mirna_importances = feature_importances[top_mirna_indices]\n",
    "\n",
    "                X_train_combined[\"combined_weighted\"] = np.dot(\n",
    "                    X_train_scaled[top_targets],\n",
    "                    top_mirna_importances / np.sum(top_mirna_importances),\n",
    "                )\n",
    "                X_test_combined[\"combined_weighted\"] = np.dot(\n",
    "                    X_test_scaled[top_targets],\n",
    "                    top_mirna_importances / np.sum(top_mirna_importances),\n",
    "                )\n",
    "\n",
    "                # PCA (First Principal Component)\n",
    "                pca = PCA(n_components=1)\n",
    "                X_train_combined[\"combined_pca\"] = pca.fit_transform(\n",
    "                    X_train_scaled[top_targets]\n",
    "                )\n",
    "                X_test_combined[\"combined_pca\"] = pca.transform(\n",
    "                    X_test_scaled[top_targets]\n",
    "                )\n",
    "\n",
    "                # LDA (First Linear Discriminant)\n",
    "                lda = LDA(n_components=1)\n",
    "                X_train_combined[\"combined_lda\"] = lda.fit_transform(\n",
    "                    X_train_scaled[top_targets], y_train_numeric\n",
    "                )  # Fit on numeric labels\n",
    "                X_test_combined[\"combined_lda\"] = lda.transform(\n",
    "                    X_test_scaled[top_targets]\n",
    "                )\n",
    "\n",
    "                # 2. ROC Analysis with Combined Scores (Raw and Scaled, All Comparisons)\n",
    "                roc_results_combined_raw = {}\n",
    "                roc_results_combined_scaled = {}\n",
    "\n",
    "                # Iterate through group comparisons\n",
    "                for i in range(len(groups)):\n",
    "                    for j in range(i + 1, len(groups)):\n",
    "                        comparison = f\"{groups[i]} vs {groups[j]}\"\n",
    "                        roc_results_combined_raw[comparison] = {}\n",
    "                        roc_results_combined_scaled[comparison] = {}\n",
    "                        # Subset data for the comparison, using y_train and y_test values for subsetting\n",
    "                        train_subset = y_train[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "                        test_subset = y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "\n",
    "                        # Raw and scaled data should also be subsetted using both X_train, X_test and also y_train and y_test indices\n",
    "                        X_train_raw_subset = X_train_raw[train_subset]\n",
    "                        X_test_raw_subset = X_test_raw[test_subset]\n",
    "\n",
    "                        X_train_scaled_subset = X_train_scaled[train_subset]\n",
    "                        X_test_scaled_subset = X_test_scaled[test_subset]\n",
    "                        # Convert y_train, and y_test labels to numeric\n",
    "                        y_train_subset = (\n",
    "                            y_train[train_subset][\"GROUP\"]\n",
    "                            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "                            .values\n",
    "                        )\n",
    "                        y_test_subset = (\n",
    "                            y_test[test_subset][\"GROUP\"]\n",
    "                            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "                            .values\n",
    "                        )\n",
    "\n",
    "                        # Combine raw values, scale and calculate ROC\n",
    "                        combined_X_train_raw = (\n",
    "                            X_train_raw_subset[top_targets]\n",
    "                            .mean(axis=1)\n",
    "                            .values.reshape(-1, 1)\n",
    "                        )\n",
    "                        combined_X_test_raw = (\n",
    "                            X_test_raw_subset[top_targets]\n",
    "                            .mean(axis=1)\n",
    "                            .values.reshape(-1, 1)\n",
    "                        )\n",
    "\n",
    "                        combined_X_train_raw_scaled = scaler.fit_transform(\n",
    "                            combined_X_train_raw\n",
    "                        )\n",
    "                        combined_X_test_raw_scaled = scaler.transform(\n",
    "                            combined_X_test_raw\n",
    "                        )\n",
    "\n",
    "                        roc_results_combined_raw[comparison][\n",
    "                            \"combined_top_raw_scaled\"\n",
    "                        ] = roc_analysis(\n",
    "                            combined_X_train_raw_scaled,\n",
    "                            combined_X_test_raw_scaled,\n",
    "                            y_train_subset,\n",
    "                            y_test_subset,\n",
    "                            \"combined_top_raw_scaled\",\n",
    "                            comparison,\n",
    "                            \"scaled\",\n",
    "                        )\n",
    "\n",
    "                        for method in combined_methods:\n",
    "                            # Subset data for current comparison, using training set for fitting, test set for transforming as for other methods.\n",
    "                            X_train_subset = X_train_combined[train_subset][\n",
    "                                method\n",
    "                            ].values.reshape(-1, 1)\n",
    "                            X_test_subset = X_test_combined[test_subset][\n",
    "                                method\n",
    "                            ].values.reshape(-1, 1)\n",
    "                            roc_results_combined_scaled[comparison][method] = (\n",
    "                                roc_analysis(\n",
    "                                    X_train_subset,\n",
    "                                    X_test_subset,\n",
    "                                    y_train_subset,\n",
    "                                    y_test_subset,\n",
    "                                    method,\n",
    "                                    comparison,\n",
    "                                    \"scaled\",\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                            # Saving ROC Results and Correlations with Clinical Parameters: (Will be in the next response due to character limitations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083b4a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants for file paths\n",
    "DATA_PATH: str = \"data/processed\"\n",
    "RESULTS_MAIN_PATH: str = \"results/main\"\n",
    "RESULTS_SUPPLEMENTARY_PATH: str = \"results/supplementary\"\n",
    "TARGET_MIRNAS: List[str] = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "def loadData() -> (\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]\n",
    "):\n",
    "    \"\"\"\n",
    "    Load and preprocess the data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - dfScaled: Scaled training data with 'GROUP' column.\n",
    "            - X_train_raw: Raw training data.\n",
    "            - X_test_raw: Raw test data.\n",
    "            - X_train_scaled: Scaled training data.\n",
    "            - X_test_scaled: Scaled test data.\n",
    "            - yTrain: Training labels.\n",
    "            - yTest: Test labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load scaled data\n",
    "        X_train_scaled: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_train_scaled.csv\")\n",
    "        )\n",
    "        X_test_scaled: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_test_scaled.csv\")\n",
    "        )\n",
    "        yTrain: pd.Series = pd.read_csv(os.path.join(DATA_PATH, \"y_train.csv\"))[\"GROUP\"]\n",
    "        yTest: pd.Series = pd.read_csv(os.path.join(DATA_PATH, \"y_test.csv\"))[\"GROUP\"]\n",
    "\n",
    "        # Load raw data\n",
    "        X_train_raw: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_train_raw.csv\")\n",
    "        )\n",
    "        X_test_raw: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_test_raw.csv\")\n",
    "        )\n",
    "\n",
    "        # Add 'GROUP' column to dfScaled\n",
    "        dfScaled: pd.DataFrame = X_train_scaled.copy()\n",
    "        dfScaled[\"GROUP\"] = yTrain.values\n",
    "        return (\n",
    "            dfScaled,\n",
    "            X_train_raw,\n",
    "            X_test_raw,\n",
    "            X_train_scaled,\n",
    "            X_test_scaled,\n",
    "            yTrain,\n",
    "            yTest,\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing expected column: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def performPCA(\n",
    "    df: pd.DataFrame, targets: List[str]\n",
    ") -> Tuple[PCA, np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Perform PCA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (List[str]): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing PCA object, PCA-transformed data, and explained variance ratio.\n",
    "    \"\"\"\n",
    "    pca: PCA = PCA()\n",
    "    pcaResult: np.ndarray = pca.fit_transform(df[targets])\n",
    "    explainedVariancePCA: pd.Series = pd.Series(pca.explained_variance_ratio_)\n",
    "\n",
    "    saveExplainedVariance(explainedVariancePCA, \"pca_explained_variance.csv\", \"pca\")\n",
    "    plotScatterPCA(pcaResult, df[\"GROUP\"], explainedVariancePCA)\n",
    "\n",
    "    return pca, pcaResult, explainedVariancePCA\n",
    "\n",
    "\n",
    "def plotScatterPCA(\n",
    "    pcaResult: np.ndarray, groups: pd.Series, explainedVariance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of PCA components.\n",
    "\n",
    "    Args:\n",
    "        pcaResult (np.ndarray): PCA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explainedVariance (pd.Series): Explained variance ratio for PCA components.\n",
    "    \"\"\"\n",
    "    nComponents: int = pcaResult.shape[1]\n",
    "    for i in range(nComponents):\n",
    "        for j in range(i + 1, nComponents):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=pcaResult[:, i],\n",
    "                y=pcaResult[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1} ({explainedVariance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Principal Component {j+1} ({explainedVariance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename: str = f\"pca_scatter_pc{i+1}_vs_pc{j+1}.png\"\n",
    "            filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "\n",
    "            savePlot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def performLDA(\n",
    "    df: pd.DataFrame, targets: List[str]\n",
    ") -> Tuple[LDA, np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Perform LDA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (List[str]): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing LDA object, LDA-transformed data, and explained variance ratio.\n",
    "    \"\"\"\n",
    "    lda: LDA = LDA()\n",
    "    ldaResult: np.ndarray = lda.fit_transform(df[targets], df[\"GROUP\"])\n",
    "    explainedVarianceLDA: pd.Series = pd.Series(lda.explained_variance_ratio_)\n",
    "\n",
    "    saveExplainedVariance(explainedVarianceLDA, \"lda_explained_variance.csv\", \"lda\")\n",
    "    plotScatterLDA(ldaResult, df[\"GROUP\"], explainedVarianceLDA)\n",
    "\n",
    "    return lda, ldaResult, explainedVarianceLDA\n",
    "\n",
    "\n",
    "def plotScatterLDA(\n",
    "    ldaResult: np.ndarray, groups: pd.Series, explainedVariance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of LDA components.\n",
    "\n",
    "    Args:\n",
    "        ldaResult (np.ndarray): LDA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explainedVariance (pd.Series): Explained variance ratio for LDA components.\n",
    "    \"\"\"\n",
    "    nComponents: int = ldaResult.shape[1]\n",
    "    for i in range(nComponents):\n",
    "        for j in range(i + 1, nComponents):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=ldaResult[:, i],\n",
    "                y=ldaResult[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "            plt.xlabel(f\"Linear Discriminant {i+1} ({explainedVariance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Linear Discriminant {j+1} ({explainedVariance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename: str = f\"lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "            filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "\n",
    "            savePlot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def saveExplainedVariance(\n",
    "    explainedVariance: pd.Series, filename: str, analysisType: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the explained variance ratios to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        explainedVariance (pd.Series): Explained variance ratios.\n",
    "        filename (str): The filename for the CSV.\n",
    "        analysisType (str): Type of analysis ('pca' or 'lda') to label components appropriately.\n",
    "    \"\"\"\n",
    "    componentLabel: str = \"PC\" if analysisType.lower() == \"pca\" else \"LD\"\n",
    "    explainedVarianceDF: pd.DataFrame = pd.DataFrame(\n",
    "        {\n",
    "            \"Component\": [\n",
    "                f\"{componentLabel}{i+1}\" for i in range(len(explainedVariance))\n",
    "            ],\n",
    "            \"Explained Variance Ratio\": explainedVariance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "    explainedVarianceDF.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def savePlot(filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to the specified filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save plot {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def saveRocResults(\n",
    "    rocResults: Dict[str, Dict[str, Dict[str, Any]]], outputBasePath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save ROC analysis results to CSV and JSON files.\n",
    "\n",
    "    Args:\n",
    "        rocResults (Dict[str, Dict[str, Dict[str, Any]]]): Nested dictionary containing ROC metrics.\n",
    "        outputBasePath (str): Base directory to save the ROC results.\n",
    "    \"\"\"\n",
    "    for dataType, comparisons in rocResults.items():\n",
    "        for comparison, methodsMetrics in comparisons.items():\n",
    "            comparisonDir: str = (\n",
    "                f\"roc_{comparison.lower().replace(' ', '_')}_{dataType}\"\n",
    "            )\n",
    "            outputDir: str = os.path.join(outputBasePath, comparisonDir)\n",
    "            os.makedirs(outputDir, exist_ok=True)\n",
    "\n",
    "            if isinstance(methodsMetrics, dict) and all(\n",
    "                isinstance(metrics, dict) for metrics in methodsMetrics.values()\n",
    "            ):\n",
    "                tempDF: pd.DataFrame = pd.DataFrame.from_dict(\n",
    "                    methodsMetrics, orient=\"index\"\n",
    "                )\n",
    "                tempDF.index.name = \"method\"\n",
    "                tempDF.reset_index(inplace=True)\n",
    "\n",
    "                csvFilename: str = f\"roc_results_{dataType}.csv\"\n",
    "                csvPath: str = os.path.join(outputDir, csvFilename)\n",
    "                tempDF.to_csv(csvPath, index=False)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Unexpected data format for comparison '{comparison}' and data type '{dataType}'. Skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    jsonPath: str = os.path.join(outputBasePath, \"roc_results_combined_all.json\")\n",
    "    try:\n",
    "        with open(jsonPath, \"w\") as fp:\n",
    "            json.dump(rocResults, fp, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save ROC results to JSON file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def performCorrelationAnalysis(\n",
    "    X_train_raw: pd.DataFrame,\n",
    "    X_test_raw: pd.DataFrame,\n",
    "    X_train_scaled: pd.DataFrame,\n",
    "    X_test_scaled: pd.DataFrame,\n",
    "    topTargets: List[str],\n",
    "    clinicalParams: List[str],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform correlation analysis for combined miRNAs and save heatmaps and matrices.\n",
    "\n",
    "    Args:\n",
    "        X_train_raw (pd.DataFrame): Raw training data.\n",
    "        X_test_raw (pd.DataFrame): Raw test data.\n",
    "        X_train_scaled (pd.DataFrame): Scaled training data.\n",
    "        X_test_scaled (pd.DataFrame): Scaled test data.\n",
    "        topTargets (List[str]): List of top target miRNA column names.\n",
    "        clinicalParams (List[str]): List of clinical parameter column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X_train_raw[\"combined_raw\"] = X_train_raw[topTargets].mean(axis=1)\n",
    "        X_test_raw[\"combined_raw\"] = X_test_raw[topTargets].mean(axis=1)\n",
    "\n",
    "        X_train_scaled[\"combined_scaled_top\"] = X_train_scaled[topTargets].mean(axis=1)\n",
    "        X_test_scaled[\"combined_scaled_top\"] = X_test_scaled[topTargets].mean(axis=1)\n",
    "\n",
    "        correlationMatrixCombinedRaw: pd.DataFrame = X_train_raw[\n",
    "            [\"combined_raw\"] + clinicalParams\n",
    "        ].corr(method=\"pearson\")\n",
    "        correlationMatrixCombinedScaled: pd.DataFrame = X_train_scaled[\n",
    "            [\"combined_scaled_top\"] + clinicalParams\n",
    "        ].corr(method=\"pearson\")\n",
    "\n",
    "        generateAndSaveHeatmap(\n",
    "            correlationMatrixCombinedRaw,\n",
    "            \"Correlation Heatmap (Combined Raw Ct Values)\",\n",
    "            os.path.join(\n",
    "                RESULTS_SUPPLEMENTARY_PATH, \"correlation_heatmap_combined_raw.png\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        generateAndSaveHeatmap(\n",
    "            correlationMatrixCombinedScaled,\n",
    "            \"Correlation Heatmap (Combined Scaled Ct Values)\",\n",
    "            os.path.join(RESULTS_MAIN_PATH, \"correlation_heatmap_combined_scaled.png\"),\n",
    "        )\n",
    "\n",
    "        correlationMatrixCombinedRaw.to_csv(\n",
    "            os.path.join(\n",
    "                RESULTS_SUPPLEMENTARY_PATH, \"correlation_matrix_combined_raw.csv\"\n",
    "            ),\n",
    "            index=True,\n",
    "        )\n",
    "        correlationMatrixCombinedScaled.to_csv(\n",
    "            os.path.join(RESULTS_MAIN_PATH, \"correlation_matrix_combined_scaled.csv\"),\n",
    "            index=True,\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing expected column: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during correlation analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generateAndSaveHeatmap(corrMatrix: pd.DataFrame, title: str, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate and save a heatmap for the given correlation matrix.\n",
    "\n",
    "    Args:\n",
    "        corrMatrix (pd.DataFrame): Correlation matrix to visualize.\n",
    "        title (str): Title of the heatmap.\n",
    "        filepath (str): Path to save the heatmap image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(corrMatrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate or save heatmap '{title}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def performROCAnalysisRaw(\n",
    "    X_train: pd.DataFrame, y_train: pd.Series, comparison: str, method: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform ROC analysis on raw data for a given comparison and method.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        comparison (str): Comparison groups (e.g., 'S vs G').\n",
    "        method (str): Machine learning method to use.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary of ROC metrics.\n",
    "    \"\"\"\n",
    "    # Implement your ROC analysis logic here\n",
    "    # For the placeholder, we'll generate random metrics\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    metrics = {\n",
    "        \"AUC\": np.random.rand(),\n",
    "        \"Accuracy\": np.random.rand(),\n",
    "        \"Sensitivity\": np.random.rand(),\n",
    "        \"Specificity\": np.random.rand(),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def performROCAnalysisScaled(\n",
    "    X_train: pd.DataFrame, y_train: pd.Series, comparison: str, method: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform ROC analysis on scaled data for a given comparison and method.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        comparison (str): Comparison groups (e.g., 'S vs G').\n",
    "        method (str): Machine learning method to use.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary of ROC metrics.\n",
    "    \"\"\"\n",
    "    # Implement your ROC analysis logic here\n",
    "    # For the placeholder, we'll generate random metrics\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "    metrics = {\n",
    "        \"AUC\": np.random.rand(),\n",
    "        \"Accuracy\": np.random.rand(),\n",
    "        \"Sensitivity\": np.random.rand(),\n",
    "        \"Specificity\": np.random.rand(),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to perform PCA, LDA, ROC analysis, and correlation analysis.\n",
    "    \"\"\"\n",
    "    os.makedirs(RESULTS_MAIN_PATH, exist_ok=True)\n",
    "    os.makedirs(RESULTS_SUPPLEMENTARY_PATH, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    (\n",
    "        dfScaled,\n",
    "        X_train_raw,\n",
    "        X_test_raw,\n",
    "        X_train_scaled,\n",
    "        X_test_scaled,\n",
    "        yTrain,\n",
    "        yTest,\n",
    "    ) = loadData()\n",
    "\n",
    "    # Perform PCA and LDA analyses\n",
    "    pca, pcaResult, explainedVariancePCA = performPCA(dfScaled, TARGET_MIRNAS)\n",
    "    lda, ldaResult, explainedVarianceLDA = performLDA(dfScaled, TARGET_MIRNAS)\n",
    "\n",
    "    # Placeholder for top targets and clinical parameters\n",
    "    topTargets: List[str] = TARGET_MIRNAS  # Update with actual top targets if different\n",
    "    clinicalParams: List[str] = [\n",
    "        \"AGE\",\n",
    "        \"SEX\",\n",
    "        \"plaque_index\",\n",
    "        \"gingival_index\",\n",
    "    ]  # Replace with actual parameters\n",
    "\n",
    "    # Perform ROC analysis and collect results\n",
    "    comparisons = [\"S vs G\", \"S vs P\", \"G vs P\"]\n",
    "    methods = [\n",
    "        \"LogisticRegression\",\n",
    "        \"RandomForest\",\n",
    "        \"SVM\",\n",
    "    ]  # Update with your actual methods\n",
    "\n",
    "    rocResultsCombinedRaw: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "    rocResultsCombinedScaled: Dict[str, Dict[str, Dict[str, Any]]] = {}\n",
    "\n",
    "    # For raw data\n",
    "    for comparison in comparisons:\n",
    "        rocResultsCombinedRaw[comparison] = {}\n",
    "        for method in methods:\n",
    "            metricsRaw = performROCAnalysisRaw(X_train_raw, yTrain, comparison, method)\n",
    "            rocResultsCombinedRaw[comparison][method] = metricsRaw\n",
    "\n",
    "    # For scaled data\n",
    "    for comparison in comparisons:\n",
    "        rocResultsCombinedScaled[comparison] = {}\n",
    "        for method in methods:\n",
    "            metricsScaled = performROCAnalysisScaled(\n",
    "                X_train_scaled, yTrain, comparison, method\n",
    "            )\n",
    "            rocResultsCombinedScaled[comparison][method] = metricsScaled\n",
    "\n",
    "    # Combine ROC results\n",
    "    rocResultsCombinedAll = {\n",
    "        \"combined_raw\": rocResultsCombinedRaw,\n",
    "        \"combined_scaled\": rocResultsCombinedScaled,\n",
    "    }\n",
    "\n",
    "    # Save ROC results\n",
    "    saveRocResults(rocResultsCombinedAll, RESULTS_MAIN_PATH)\n",
    "\n",
    "    # Perform correlation analysis\n",
    "    performCorrelationAnalysis(\n",
    "        X_train_raw,\n",
    "        X_test_raw,\n",
    "        X_train_scaled,\n",
    "        X_test_scaled,\n",
    "        topTargets,\n",
    "        clinicalParams,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bb9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected data format for comparison 'S vs G' and data type 'combined_raw'. Skipping...\n",
      "Unexpected data format for comparison 'S vs P' and data type 'combined_raw'. Skipping...\n",
      "Unexpected data format for comparison 'G vs P' and data type 'combined_raw'. Skipping...\n",
      "Unexpected data format for comparison 'S vs G' and data type 'combined_scaled'. Skipping...\n",
      "Unexpected data format for comparison 'S vs P' and data type 'combined_scaled'. Skipping...\n",
      "Unexpected data format for comparison 'G vs P' and data type 'combined_scaled'. Skipping...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, Any, Tuple\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Constants for file paths\n",
    "DATA_PATH: str = \"data/processed\"\n",
    "RESULTS_MAIN_PATH: str = \"results/main\"\n",
    "RESULTS_SUPPLEMENTARY_PATH: str = \"results/supplementary\"\n",
    "TARGET_MIRNAS: list[str] = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess the scaled training data by adding the 'GROUP' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The scaled training data with the 'GROUP' column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_scaled: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_train_scaled.csv\")\n",
    "        )\n",
    "        y_train: pd.DataFrame = pd.read_csv(os.path.join(DATA_PATH, \"y_train.csv\"))\n",
    "\n",
    "        if \"GROUP\" not in y_train.columns:\n",
    "            raise KeyError(\"The 'GROUP' column is missing from y_train.csv\")\n",
    "\n",
    "        df_scaled[\"GROUP\"] = y_train[\"GROUP\"].values\n",
    "        return df_scaled\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def perform_pca(\n",
    "    df: pd.DataFrame, targets: list[str]\n",
    ") -> Tuple[PCA, np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Perform PCA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list[str]): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[PCA, np.ndarray, pd.Series]: PCA object, PCA-transformed data, and explained variance ratio.\n",
    "    \"\"\"\n",
    "    pca: PCA = PCA()\n",
    "    pca_result: np.ndarray = pca.fit_transform(df[targets])\n",
    "    explained_variance_pca: pd.Series = pd.Series(pca.explained_variance_ratio_)\n",
    "\n",
    "    save_explained_variance(explained_variance_pca, \"pca_explained_variance.csv\", \"pca\")\n",
    "    plot_scatter_pca(pca_result, df[\"GROUP\"], explained_variance_pca)\n",
    "\n",
    "    return pca, pca_result, explained_variance_pca\n",
    "\n",
    "\n",
    "def plot_scatter_pca(\n",
    "    pca_result: np.ndarray, groups: pd.Series, explained_variance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of PCA components.\n",
    "\n",
    "    Args:\n",
    "        pca_result (np.ndarray): PCA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explained_variance (pd.Series): Explained variance ratio for PCA components.\n",
    "    \"\"\"\n",
    "    n_components: int = pca_result.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=pca_result[:, i],\n",
    "                y=pca_result[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1} ({explained_variance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Principal Component {j+1} ({explained_variance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename: str = f\"pca_scatter_pc{i+1}_vs_pc{j+1}.png\"\n",
    "            filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "\n",
    "            save_plot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def perform_lda(\n",
    "    df: pd.DataFrame, targets: list[str]\n",
    ") -> Tuple[LDA, np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Perform LDA on the target miRNAs and visualize the results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The scaled training data with 'GROUP'.\n",
    "        targets (list[str]): List of target miRNA column names.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[LDA, np.ndarray, pd.Series]: LDA object, LDA-transformed data, and explained variance ratio.\n",
    "    \"\"\"\n",
    "    lda: LDA = LDA()\n",
    "    lda_result: np.ndarray = lda.fit_transform(df[targets], df[\"GROUP\"])\n",
    "    explained_variance_lda: pd.Series = pd.Series(lda.explained_variance_ratio_)\n",
    "\n",
    "    save_explained_variance(explained_variance_lda, \"lda_explained_variance.csv\", \"lda\")\n",
    "    plot_scatter_lda(lda_result, df[\"GROUP\"], explained_variance_lda)\n",
    "\n",
    "    return lda, lda_result, explained_variance_lda\n",
    "\n",
    "\n",
    "def plot_scatter_lda(\n",
    "    lda_result: np.ndarray, groups: pd.Series, explained_variance: pd.Series\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save pairwise 2D scatter plots of LDA components.\n",
    "\n",
    "    Args:\n",
    "        lda_result (np.ndarray): LDA-transformed data.\n",
    "        groups (pd.Series): The 'GROUP' labels.\n",
    "        explained_variance (pd.Series): Explained variance ratio for LDA components.\n",
    "    \"\"\"\n",
    "    n_components: int = lda_result.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=lda_result[:, i],\n",
    "                y=lda_result[:, j],\n",
    "                hue=groups,\n",
    "                palette=\"viridis\",\n",
    "                edgecolor=\"k\",\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "            plt.xlabel(f\"Linear Discriminant {i+1} ({explained_variance[i]:.2%})\")\n",
    "            plt.ylabel(f\"Linear Discriminant {j+1} ({explained_variance[j]:.2%})\")\n",
    "            plt.legend(title=\"Group\")\n",
    "            plt.tight_layout()\n",
    "\n",
    "            filename: str = f\"lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "            filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "\n",
    "            save_plot(filepath)\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def save_explained_variance(\n",
    "    explained_variance: pd.Series, filename: str, analysis_type: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save the explained variance ratios to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        explained_variance (pd.Series): Explained variance ratios.\n",
    "        filename (str): The filename for the CSV.\n",
    "        analysis_type (str): Type of analysis ('pca' or 'lda') to label components appropriately.\n",
    "    \"\"\"\n",
    "    component_label: str = \"PC\" if analysis_type.lower() == \"pca\" else \"LD\"\n",
    "    explained_variance_df: pd.DataFrame = pd.DataFrame(\n",
    "        {\n",
    "            \"Component\": [\n",
    "                f\"{component_label}{i+1}\" for i in range(len(explained_variance))\n",
    "            ],\n",
    "            \"Explained Variance Ratio\": explained_variance,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    filepath: str = os.path.join(RESULTS_SUPPLEMENTARY_PATH, filename)\n",
    "    explained_variance_df.to_csv(filepath, index=False)\n",
    "\n",
    "\n",
    "def save_plot(filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the current matplotlib plot to the specified filepath.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path where the plot will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.savefig(filepath, dpi=300)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save plot {filepath}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_roc_results(\n",
    "    roc_results: Dict[str, Dict[str, Dict[str, Any]]], output_base_path: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save ROC analysis results to CSV and JSON files.\n",
    "\n",
    "    Args:\n",
    "        roc_results (Dict[str, Dict[str, Dict[str, Any]]]): Nested dictionary containing ROC metrics.\n",
    "        output_base_path (str): Base directory to save the ROC results.\n",
    "    \"\"\"\n",
    "    for data_type, comparisons in roc_results.items():\n",
    "        for comparison, methods_metrics in comparisons.items():\n",
    "            comparison_dir: str = (\n",
    "                f\"roc_{comparison.lower().replace(' ', '_')}_{data_type}\"\n",
    "            )\n",
    "            output_dir: str = os.path.join(output_base_path, comparison_dir)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            if isinstance(methods_metrics, dict) and all(\n",
    "                isinstance(metrics, dict) for metrics in methods_metrics.values()\n",
    "            ):\n",
    "                temp_df: pd.DataFrame = pd.DataFrame.from_dict(\n",
    "                    methods_metrics, orient=\"index\"\n",
    "                )\n",
    "                temp_df.index.name = \"method\"\n",
    "                temp_df.reset_index(inplace=True)\n",
    "\n",
    "                csv_filename: str = f\"roc_results_{data_type}.csv\"\n",
    "                csv_path: str = os.path.join(output_dir, csv_filename)\n",
    "                temp_df.to_csv(csv_path, index=False)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Unexpected data format for comparison '{comparison}' and data type '{data_type}'. Skipping...\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "    json_path: str = os.path.join(output_base_path, \"roc_results_combined_all.json\")\n",
    "    try:\n",
    "        with open(json_path, \"w\") as fp:\n",
    "            json.dump(roc_results, fp, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save ROC results to JSON file: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def perform_correlation_analysis(\n",
    "    X_train_raw: pd.DataFrame,\n",
    "    X_test_raw: pd.DataFrame,\n",
    "    X_train_scaled: pd.DataFrame,\n",
    "    X_test_scaled: pd.DataFrame,\n",
    "    top_targets: list[str],\n",
    "    clinical_params: list[str],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform correlation analysis for combined miRNAs and save heatmaps and matrices.\n",
    "\n",
    "    Args:\n",
    "        X_train_raw (pd.DataFrame): Raw scaled training data.\n",
    "        X_test_raw (pd.DataFrame): Raw scaled test data.\n",
    "        X_train_scaled (pd.DataFrame): Scaled training data.\n",
    "        X_test_scaled (pd.DataFrame): Scaled test data.\n",
    "        top_targets (list[str]): List of top target miRNA column names.\n",
    "        clinical_params (list[str]): List of clinical parameter column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X_train_raw[\"combined_raw\"] = X_train_raw[top_targets].mean(axis=1)\n",
    "        X_test_raw[\"combined_raw\"] = X_test_raw[top_targets].mean(axis=1)\n",
    "\n",
    "        X_train_scaled[\"combined_scaled_top\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "        X_test_scaled[\"combined_scaled_top\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "        correlation_matrix_combined_raw: pd.DataFrame = X_train_raw[\n",
    "            [\"combined_raw\"] + clinical_params\n",
    "        ].corr(method=\"pearson\")\n",
    "        correlation_matrix_combined_scaled: pd.DataFrame = X_train_scaled[\n",
    "            [\"combined_scaled_top\"] + clinical_params\n",
    "        ].corr(method=\"pearson\")\n",
    "\n",
    "        generate_and_save_heatmap(\n",
    "            correlation_matrix_combined_raw,\n",
    "            \"Correlation Heatmap (Combined Raw Ct Values)\",\n",
    "            os.path.join(\n",
    "                RESULTS_SUPPLEMENTARY_PATH, \"correlation_heatmap_combined_raw.png\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        generate_and_save_heatmap(\n",
    "            correlation_matrix_combined_scaled,\n",
    "            \"Correlation Heatmap (Combined Scaled Ct Values)\",\n",
    "            os.path.join(RESULTS_MAIN_PATH, \"correlation_heatmap_combined_scaled.png\"),\n",
    "        )\n",
    "\n",
    "        correlation_matrix_combined_raw.to_csv(\n",
    "            os.path.join(\n",
    "                RESULTS_SUPPLEMENTARY_PATH, \"correlation_matrix_combined_raw.csv\"\n",
    "            ),\n",
    "            index=True,\n",
    "        )\n",
    "        correlation_matrix_combined_scaled.to_csv(\n",
    "            os.path.join(RESULTS_MAIN_PATH, \"correlation_matrix_combined_scaled.csv\"),\n",
    "            index=True,\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing expected column: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during correlation analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def generate_and_save_heatmap(\n",
    "    corr_matrix: pd.DataFrame, title: str, filepath: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate and save a heatmap for the given correlation matrix.\n",
    "\n",
    "    Args:\n",
    "        corr_matrix (pd.DataFrame): Correlation matrix to visualize.\n",
    "        title (str): Title of the heatmap.\n",
    "        filepath (str): Path to save the heatmap image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate or save heatmap '{title}': {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to perform PCA, LDA, ROC analysis, and correlation analysis.\n",
    "    \"\"\"\n",
    "    os.makedirs(RESULTS_MAIN_PATH, exist_ok=True)\n",
    "    os.makedirs(RESULTS_SUPPLEMENTARY_PATH, exist_ok=True)\n",
    "\n",
    "    df_scaled: pd.DataFrame = load_data()\n",
    "\n",
    "    pca, pca_result, explained_variance_pca = perform_pca(df_scaled, TARGET_MIRNAS)\n",
    "    lda, lda_result, explained_variance_lda = perform_lda(df_scaled, TARGET_MIRNAS)\n",
    "\n",
    "    # ... (previous code for combining miRNAs and ROC analysis) ...\n",
    "\n",
    "    roc_results_combined_all: Dict[str, Dict[str, Dict[str, Any]]] = {\n",
    "        \"combined_raw\": roc_results_combined_raw,\n",
    "        \"combined_scaled\": roc_results_combined_scaled,\n",
    "    }\n",
    "\n",
    "    save_roc_results(roc_results_combined_all, RESULTS_MAIN_PATH)\n",
    "\n",
    "    perform_correlation_analysis(\n",
    "        X_train_raw,\n",
    "        X_test_raw,\n",
    "        X_train_scaled,\n",
    "        X_test_scaled,\n",
    "        top_targets,\n",
    "        clinical_params,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc478bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique groups in training data: ['G' 'S' 'P']\n",
      "Valid comparisons: ['G vs S', 'G vs P', 'S vs P']\n",
      "Starting ROC analysis for comparison: G vs S\n",
      "Training samples: 57, Testing samples: 15\n",
      "Training labels: [0 1], Testing labels: [1 0]\n",
      "Performing ROC analysis for G vs S using pocket_depth (scaled data)...\n",
      "Completed LogisticRegression for G vs S using pocket_depth. AUC: 0.5893\n",
      "Completed RandomForest for G vs S using pocket_depth. AUC: 0.5000\n",
      "Completed SVM for G vs S using pocket_depth. AUC: 0.4821\n",
      "Performing ROC analysis for G vs S using bleeding_on_probing (scaled data)...\n",
      "Completed LogisticRegression for G vs S using bleeding_on_probing. AUC: 1.0000\n",
      "Completed RandomForest for G vs S using bleeding_on_probing. AUC: 1.0000\n",
      "Completed SVM for G vs S using bleeding_on_probing. AUC: 1.0000\n",
      "Performing ROC analysis for G vs S using combinedTopMiRNA (scaled data)...\n",
      "Completed LogisticRegression for G vs S using combinedTopMiRNA. AUC: 0.4464\n",
      "Completed RandomForest for G vs S using combinedTopMiRNA. AUC: 0.7054\n",
      "Completed SVM for G vs S using combinedTopMiRNA. AUC: 0.6786\n",
      "Completed ROC analysis for G vs S\n",
      "\n",
      "ROC analysis results saved to results/main\\roc_results_combined_clinical.json\n",
      "Starting ROC analysis for comparison: G vs P\n",
      "Training samples: 58, Testing samples: 14\n",
      "Training labels: [0 1], Testing labels: [1 0]\n",
      "Performing ROC analysis for G vs P using pocket_depth (scaled data)...\n",
      "Completed LogisticRegression for G vs P using pocket_depth. AUC: 1.0000\n",
      "Completed RandomForest for G vs P using pocket_depth. AUC: 0.9898\n",
      "Completed SVM for G vs P using pocket_depth. AUC: 1.0000\n",
      "Performing ROC analysis for G vs P using bleeding_on_probing (scaled data)...\n",
      "Completed LogisticRegression for G vs P using bleeding_on_probing. AUC: 0.8776\n",
      "Completed RandomForest for G vs P using bleeding_on_probing. AUC: 0.7959\n",
      "Completed SVM for G vs P using bleeding_on_probing. AUC: 0.7755\n",
      "Performing ROC analysis for G vs P using combinedTopMiRNA (scaled data)...\n",
      "Completed LogisticRegression for G vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed RandomForest for G vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed SVM for G vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed ROC analysis for G vs P\n",
      "\n",
      "ROC analysis results saved to results/main\\roc_results_combined_clinical.json\n",
      "Starting ROC analysis for comparison: S vs P\n",
      "Training samples: 57, Testing samples: 15\n",
      "Training labels: [0 1], Testing labels: [1 0]\n",
      "Performing ROC analysis for S vs P using pocket_depth (scaled data)...\n",
      "Completed LogisticRegression for S vs P using pocket_depth. AUC: 1.0000\n",
      "Completed RandomForest for S vs P using pocket_depth. AUC: 1.0000\n",
      "Completed SVM for S vs P using pocket_depth. AUC: 1.0000\n",
      "Performing ROC analysis for S vs P using bleeding_on_probing (scaled data)...\n",
      "Completed LogisticRegression for S vs P using bleeding_on_probing. AUC: 1.0000\n",
      "Completed RandomForest for S vs P using bleeding_on_probing. AUC: 1.0000\n",
      "Completed SVM for S vs P using bleeding_on_probing. AUC: 1.0000\n",
      "Performing ROC analysis for S vs P using combinedTopMiRNA (scaled data)...\n",
      "Completed LogisticRegression for S vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed RandomForest for S vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed SVM for S vs P using combinedTopMiRNA. AUC: 1.0000\n",
      "Completed ROC analysis for S vs P\n",
      "\n",
      "ROC analysis results saved to results/main\\roc_results_combined_clinical.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Any, Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Constants for file paths\n",
    "DATA_PATH: str = \"data/processed\"\n",
    "RESULTS_MAIN_PATH: str = \"results/main\"\n",
    "RESULTS_SUPPLEMENTARY_PATH: str = \"results/supplementary\"\n",
    "\n",
    "TARGET_MIRNAS: List[str] = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "def load_data() -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load and preprocess the data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - df_scaled: Scaled training data with 'GROUP' column.\n",
    "            - X_train_raw: Raw training data.\n",
    "            - X_test_raw: Raw test data.\n",
    "            - X_train_scaled: Scaled training data.\n",
    "            - X_test_scaled: Scaled test data.\n",
    "            - y_train: Training labels.\n",
    "            - y_test: Test labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load scaled data\n",
    "        X_train_scaled: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_train_scaled.csv\")\n",
    "        )\n",
    "        X_test_scaled: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_test_scaled.csv\")\n",
    "        )\n",
    "        y_train: pd.Series = pd.read_csv(os.path.join(DATA_PATH, \"y_train.csv\"))[\n",
    "            \"GROUP\"\n",
    "        ]\n",
    "        y_test: pd.Series = pd.read_csv(os.path.join(DATA_PATH, \"y_test.csv\"))[\"GROUP\"]\n",
    "\n",
    "        # Load raw data\n",
    "        X_train_raw: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_train_raw.csv\")\n",
    "        )\n",
    "        X_test_raw: pd.DataFrame = pd.read_csv(\n",
    "            os.path.join(DATA_PATH, \"X_test_raw.csv\")\n",
    "        )\n",
    "\n",
    "        # Add 'GROUP' column to df_scaled\n",
    "        df_scaled: pd.DataFrame = X_train_scaled.copy()\n",
    "        df_scaled[\"GROUP\"] = y_train.values\n",
    "\n",
    "        return (\n",
    "            df_scaled,\n",
    "            X_train_raw,\n",
    "            X_test_raw,\n",
    "            X_train_scaled,\n",
    "            X_test_scaled,\n",
    "            y_train,\n",
    "            y_test,\n",
    "        )\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing expected column: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def perform_roc_analysis(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    model_name: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Perform ROC analysis for a given model.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Testing labels.\n",
    "        model_name (str): Name of the model to use.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing FPR, TPR, and AUC.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the model based on the model name\n",
    "        if model_name == \"LogisticRegression\":\n",
    "            model = LogisticRegression(solver=\"liblinear\")\n",
    "        elif model_name == \"RandomForest\":\n",
    "            model = RandomForestClassifier(n_estimators=100)\n",
    "        elif model_name == \"SVM\":\n",
    "            model = SVC(probability=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict probabilities\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate ROC curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        # Convert NumPy arrays to lists for JSON serialization\n",
    "        return {\"FPR\": fpr.tolist(), \"TPR\": tpr.tolist(), \"AUC\": roc_auc}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in perform_roc_analysis for model {model_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def roc_analysis(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    y_test: pd.Series,\n",
    "    target_name: str,\n",
    "    comparison_name: str,\n",
    "    data_type: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Wrapper function to perform ROC analysis and handle debugging.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Testing labels.\n",
    "        target_name (str): Name of the target feature.\n",
    "        comparison_name (str): Description of the comparison.\n",
    "        data_type (str): Type of data ('raw' or 'scaled').\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: ROC analysis results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\n",
    "            f\"Performing ROC analysis for {comparison_name} using {target_name} ({data_type} data)...\"\n",
    "        )\n",
    "        models = [\"LogisticRegression\", \"RandomForest\", \"SVM\"]\n",
    "        results = {}\n",
    "        for model in models:\n",
    "            metrics = perform_roc_analysis(X_train, X_test, y_train, y_test, model)\n",
    "            results[model] = metrics\n",
    "            print(\n",
    "                f\"Completed {model} for {comparison_name} using {target_name}. AUC: {metrics['AUC']:.4f}\"\n",
    "            )\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error in roc_analysis for {comparison_name} with target {target_name}: {e}\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to perform data loading, preprocessing, and ROC analysis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(RESULTS_MAIN_PATH, exist_ok=True)\n",
    "        os.makedirs(RESULTS_SUPPLEMENTARY_PATH, exist_ok=True)\n",
    "\n",
    "        # Load data\n",
    "        (\n",
    "            df_scaled,\n",
    "            X_train_raw,\n",
    "            X_test_raw,\n",
    "            X_train_scaled,\n",
    "            X_test_scaled,\n",
    "            y_train,\n",
    "            y_test,\n",
    "        ) = load_data()\n",
    "\n",
    "        # Define unique groups\n",
    "        unique_groups = y_train.unique()\n",
    "        print(f\"Unique groups in training data: {unique_groups}\")\n",
    "\n",
    "        # Define top miRNAs and clinical parameters\n",
    "        top_mirnas = TARGET_MIRNAS  # Update if different\n",
    "        top_clinical_params = [\n",
    "            \"pocket_depth\",\n",
    "            \"bleeding_on_probing\",\n",
    "            # Add other clinical parameters as needed\n",
    "        ]\n",
    "\n",
    "        # Initialize ROC results dictionary\n",
    "        roc_results_combined_clinical: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        # Define valid comparisons (exclude comparisons within the same group)\n",
    "        valid_comparisons = []\n",
    "        for i in range(len(unique_groups)):\n",
    "            for j in range(i + 1, len(unique_groups)):\n",
    "                comparison_name = f\"{unique_groups[i]} vs {unique_groups[j]}\"\n",
    "                valid_comparisons.append(\n",
    "                    (unique_groups[i], unique_groups[j], comparison_name)\n",
    "                )\n",
    "\n",
    "        print(f\"Valid comparisons: {[name for _, _, name in valid_comparisons]}\")\n",
    "\n",
    "        for group1, group2, comparison_name in valid_comparisons:\n",
    "            print(f\"Starting ROC analysis for comparison: {comparison_name}\")\n",
    "\n",
    "            # Subset training data for current comparison\n",
    "            train_indices = (y_train == group1) | (y_train == group2)\n",
    "            X_train_subset_scaled = X_train_scaled[train_indices]\n",
    "            y_train_subset = y_train[train_indices].apply(\n",
    "                lambda x: 0 if x == group1 else 1\n",
    "            )\n",
    "\n",
    "            # Subset testing data for current comparison\n",
    "            test_indices = (y_test == group1) | (y_test == group2)\n",
    "            X_test_subset_scaled = X_test_scaled[test_indices]\n",
    "            y_test_subset = y_test[test_indices].apply(\n",
    "                lambda x: 0 if x == group1 else 1\n",
    "            )\n",
    "\n",
    "            # Validate consistent sample sizes and presence of both classes\n",
    "            unique_train_labels = y_train_subset.unique()\n",
    "            unique_test_labels = y_test_subset.unique()\n",
    "            print(\n",
    "                f\"Training samples: {len(X_train_subset_scaled)}, Testing samples: {len(X_test_subset_scaled)}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Training labels: {unique_train_labels}, Testing labels: {unique_test_labels}\"\n",
    "            )\n",
    "\n",
    "            if len(unique_train_labels) < 2 or len(unique_test_labels) < 2:\n",
    "                print(\n",
    "                    f\"Skipping ROC analysis for {comparison_name} due to insufficient classes.\\n\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            roc_results_combined_clinical[comparison_name] = {}\n",
    "\n",
    "            # ROC Analysis using only Clinical Parameters\n",
    "            for param in top_clinical_params:\n",
    "                if param not in X_train_subset_scaled.columns:\n",
    "                    print(\n",
    "                        f\"Parameter '{param}' not found in X_train_subset_scaled. Skipping...\"\n",
    "                    )\n",
    "                    continue\n",
    "                try:\n",
    "                    roc_result = roc_analysis(\n",
    "                        X_train_subset_scaled[[param]],\n",
    "                        X_test_subset_scaled[[param]],\n",
    "                        y_train_subset,\n",
    "                        y_test_subset,\n",
    "                        param,\n",
    "                        comparison_name,\n",
    "                        \"scaled\",\n",
    "                    )\n",
    "                    roc_results_combined_clinical[comparison_name][param] = roc_result\n",
    "                except Exception as e:\n",
    "                    print(\n",
    "                        f\"Failed ROC analysis for parameter '{param}' in {comparison_name}: {e}\"\n",
    "                    )\n",
    "\n",
    "            # Combine Scaled Top miRNAs for the Current Comparison\n",
    "            try:\n",
    "                combined_top_miRNA_train = (\n",
    "                    X_train_subset_scaled[top_mirnas].mean(axis=1).values.reshape(-1, 1)\n",
    "                )\n",
    "                combined_top_miRNA_test = (\n",
    "                    X_test_subset_scaled[top_mirnas].mean(axis=1).values.reshape(-1, 1)\n",
    "                )\n",
    "\n",
    "                combined_top_miRNA_train_df = pd.DataFrame(\n",
    "                    combined_top_miRNA_train, columns=[\"combinedTopMiRNA\"]\n",
    "                )\n",
    "                combined_top_miRNA_test_df = pd.DataFrame(\n",
    "                    combined_top_miRNA_test, columns=[\"combinedTopMiRNA\"]\n",
    "                )\n",
    "\n",
    "                roc_combined_top_miRNA = roc_analysis(\n",
    "                    combined_top_miRNA_train_df,\n",
    "                    combined_top_miRNA_test_df,\n",
    "                    y_train_subset,\n",
    "                    y_test_subset,\n",
    "                    \"combinedTopMiRNA\",\n",
    "                    comparison_name,\n",
    "                    \"scaled\",\n",
    "                )\n",
    "                roc_results_combined_clinical[comparison_name][\n",
    "                    \"combinedTopMiRNA\"\n",
    "                ] = roc_combined_top_miRNA\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    f\"Failed ROC analysis for combinedTopMiRNA in {comparison_name}: {e}\"\n",
    "                )\n",
    "\n",
    "            print(f\"Completed ROC analysis for {comparison_name}\\n\")\n",
    "\n",
    "            # Save ROC Results\n",
    "            roc_results_path = os.path.join(\n",
    "                RESULTS_MAIN_PATH, \"roc_results_combined_clinical.json\"\n",
    "            )\n",
    "            try:\n",
    "                with open(roc_results_path, \"w\") as f:\n",
    "                    json.dump(roc_results_combined_clinical, f, indent=4)\n",
    "                print(f\"ROC analysis results saved to {roc_results_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save ROC results to {roc_results_path}: {e}\")\n",
    "                raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the main function: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
