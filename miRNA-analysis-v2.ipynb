{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# miRNA Analysis v2\n",
    "\n",
    "This notebook performs miRNA analysis with updated methods and additional steps as outlined in the miRNA-Manuscript-Revision.md file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data per column:\n",
      "GROUP                      0\n",
      "SEX                        0\n",
      "AGE                        0\n",
      "plaque_index               0\n",
      "gingival_index             0\n",
      "pocket_depth               0\n",
      "bleeding_on_probing        0\n",
      "number_of_missing_teeth    0\n",
      "mean_mir146a               0\n",
      "mean_mir146b               0\n",
      "mean_mir155                0\n",
      "mean_mir203                0\n",
      "mean_mir223                0\n",
      "mean_mir381p               0\n",
      "mean_GAPDH                 0\n",
      "dtype: int64\n",
      "No missing values found.\n",
      "Data types after conversion:\n",
      "GROUP                      category\n",
      "SEX                        category\n",
      "AGE                           int64\n",
      "plaque_index                float64\n",
      "gingival_index              float64\n",
      "pocket_depth                float64\n",
      "bleeding_on_probing         float64\n",
      "number_of_missing_teeth       int64\n",
      "mean_mir146a                float64\n",
      "mean_mir146b                float64\n",
      "mean_mir155                 float64\n",
      "mean_mir203                 float64\n",
      "mean_mir223                 float64\n",
      "mean_mir381p                float64\n",
      "mean_GAPDH                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# 1. Load the raw data\n",
    "\n",
    "\n",
    "\n",
    "data_path = (\n",
    "    \"data/raw/Kasim2024-son-veri.csv\"  # Correct path based on repository structure\n",
    ")\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "\n",
    "# 2. Check for missing data\n",
    "\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Missing data per column:\\n{missing_data}\")  # Report findings\n",
    "\n",
    "\n",
    "# Handle missing data (if any). Examples (choose one if needed):\n",
    "\n",
    "\n",
    "if missing_data.sum() > 0:\n",
    "\n",
    "    # Mean imputation:\n",
    "\n",
    "    # df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "    # Median imputation:\n",
    "\n",
    "    # df.fillna(df.median(), inplace=True)\n",
    "\n",
    "    print(\"Missing values imputed using [chosen method].\")  # Explain chosen method\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "\n",
    "\n",
    "# 3. Convert 'SEX' and 'GROUP'\n",
    "\n",
    "\n",
    "df[\"SEX\"] = df[\"SEX\"].map({\"F\": 0, \"M\": 1}).astype(\"category\")  # Female: 0, Male: 1\n",
    "\n",
    "\n",
    "\n",
    "df[\"GROUP\"] = df[\"GROUP\"].astype(\"category\")\n",
    "\n",
    "\n",
    "print(f\"Data types after conversion:\\n{df.dtypes}\")\n",
    "\n",
    "\n",
    "\n",
    "# 4. Robust scaling normalization (on the entire dataset initially)\n",
    "\n",
    "\n",
    "targets = [\n",
    "\n",
    "    \"mean_mir146a\",\n",
    "\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "df[targets] = scaler.fit_transform(df[targets])\n",
    "\n",
    "\n",
    "\n",
    "# 5. Split data into training and testing sets (after normalization)\n",
    "\n",
    "\n",
    "X = df[\n",
    "    targets\n",
    "\n",
    "    + [\n",
    "\n",
    "        \"SEX\",\n",
    "        \"AGE\",\n",
    "        \"plaque_index\",\n",
    "        \"gingival_index\",\n",
    "        \"pocket_depth\",\n",
    "        \"bleeding_on_probing\",\n",
    "        \"number_of_missing_teeth\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "y = df[\"GROUP\"]\n",
    "\n",
    "\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")  # Corrected to stratify by your group label\n",
    "\n",
    "\n",
    "y_train_numeric = y_train.cat.codes\n",
    "\n",
    "\n",
    "y_test_numeric = y_test.cat.codes\n",
    "\n",
    "\n",
    "\n",
    "# Scaling the training data and saving scaling parameters for testing set\n",
    "\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "\n",
    "\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Re-initializing the scaler to ensure it's not contaminated by test data\n",
    "\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "\n",
    "X_train_scaled[targets] = scaler.fit_transform(X_train[targets])\n",
    "\n",
    "\n",
    "X_test_scaled[targets] = scaler.transform(X_test[targets])  # Apply to test data\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the 'src' directory exists\n",
    "os.makedirs(\"src\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Saving the scaling parameters so that if we want to apply the same procedure to unseen data, we can load the scaler and transform the data directly using this pickle file.\n",
    "\n",
    "\n",
    "\n",
    "with open(\"src/scaler.pkl\", \"wb\") as f:\n",
    "\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "\n",
    "# Create directories if they don't exist\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save preprocessed data (both raw and scaled training/test sets)\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv(\"data/processed/normalized_mirna_data.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Save raw and scaled train and test datasets into their respective directories.\n",
    "\n",
    "\n",
    "\n",
    "X_train.to_csv(\"data/processed/X_train_raw.csv\", index=False)\n",
    "\n",
    "\n",
    "X_test.to_csv(\"data/processed/X_test_raw.csv\", index=False)\n",
    "\n",
    "\n",
    "y_train.to_csv(\"data/processed/y_train.csv\", index=False)\n",
    "\n",
    "\n",
    "y_test.to_csv(\"data/processed/y_test.csv\", index=False)\n",
    "\n",
    "\n",
    "X_train_scaled.to_csv(\"data/processed/X_train_scaled.csv\", index=False)\n",
    "\n",
    "\n",
    "X_test_scaled.to_csv(\"data/processed/X_test_scaled.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "\n",
    "X_train.describe().to_csv(\"results/main/descriptive_stats_raw_train.csv\")\n",
    "\n",
    "\n",
    "X_test.describe().to_csv(\"results/main/descriptive_stats_raw_test.csv\")\n",
    "\n",
    "\n",
    "X_train_scaled.describe().to_csv(\"results/main/descriptive_stats_scaled_train.csv\")\n",
    "\n",
    "\n",
    "X_test_scaled.describe().to_csv(\"results/main/descriptive_stats_scaled_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results for GAPDH across groups: F-value = 190.1558, P-value = 0.0000\n",
      "Levene's test results: Statistic = 4.5712, P-value = 0.0125\n",
      "Tukey HSD post hoc test results for GAPDH:\n",
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "====================================================\n",
      "group1 group2 meandiff p-adj   lower   upper  reject\n",
      "----------------------------------------------------\n",
      "     G      P  -4.5173    0.0 -5.1983 -3.8363   True\n",
      "     G      S    0.587 0.1056 -0.0939   1.268  False\n",
      "     P      S   5.1044    0.0  4.4234  5.7853   True\n",
      "----------------------------------------------------\n",
      "Pearson correlation between GAPDH and plaque_index: -0.6614, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and gingival_index: -0.6050, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and pocket_depth: -0.8144, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and bleeding_on_probing: -0.7548, P-value = 0.0000\n",
      "Pearson correlation between GAPDH and number_of_missing_teeth: -0.2378, P-value = 0.0132\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "\n",
    "\n",
    "# Load the data from the CSV file into a pandas DataFrame\n",
    "\n",
    "\n",
    "data_path = r\"data/raw/Kasim2024-son-veri.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "\n",
    "\n",
    "# Check if 'GROUP' column exists\n",
    "if \"GROUP\" not in df.columns:\n",
    "    raise KeyError(\"'GROUP' column is missing from the DataFrame\")\n",
    "\n",
    "# 1. ANOVA and post hoc tests on GAPDH\n",
    "groups = df[\"GROUP\"].unique()\n",
    "\n",
    "\n",
    "gapdh_data = [df[\"mean_GAPDH\"][df[\"GROUP\"] == group] for group in groups]\n",
    "\n",
    "\n",
    "fvalue, pvalue = stats.f_oneway(*gapdh_data)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"ANOVA results for GAPDH across groups: F-value = {fvalue:.4f}, P-value = {pvalue:.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Perform Levene's test for homogeneity of variances\n",
    "levene_stat, levene_p = stats.levene(*gapdh_data)\n",
    "\n",
    "\n",
    "print(f\"Levene's test results: Statistic = {levene_stat:.4f}, P-value = {levene_p:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Perform post hoc Tukey HSD test if ANOVA is significant\n",
    "if pvalue < 0.05:\n",
    "\n",
    "\n",
    "    tukey_result = pairwise_tukeyhsd(df[\"mean_GAPDH\"], df[\"GROUP\"], alpha=0.05)\n",
    "\n",
    "    print(\"Tukey HSD post hoc test results for GAPDH:\")\n",
    "\n",
    "    print(tukey_result)\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    print(\"ANOVA not significant; no post hoc test performed.\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate Pearson correlations between GAPDH Ct and clinical parameters\n",
    "\n",
    "\n",
    "clinical_parameters = [\n",
    "    \"plaque_index\",\n",
    "    \"gingival_index\",\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "    \"number_of_missing_teeth\",\n",
    "]\n",
    "\n",
    "\n",
    "correlations = {}\n",
    "for parameter in clinical_parameters:\n",
    "    correlation, p_value = stats.pearsonr(df[\"mean_GAPDH\"], df[parameter])\n",
    "\n",
    "    correlations[parameter] = {\"correlation\": correlation, \"p_value\": p_value}\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"Pearson correlation between GAPDH and {parameter}: {correlation:.4f}, P-value = {p_value:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Save results and justification for raw Ct use\n",
    "\n",
    "\n",
    "gapdh_results = pd.DataFrame({\"ANOVA_F\": [fvalue], \"ANOVA_p\": [pvalue]})\n",
    "\n",
    "\n",
    "gapdh_results.to_csv(\"results/supplementary/gapdh_analysis_results.csv\", index=False)\n",
    "correlations_df = pd.DataFrame(correlations).T\n",
    "\n",
    "\n",
    "correlations_df.to_csv(\"results/supplementary/gapdh_correlations.csv\", index=True)\n",
    "\n",
    "\n",
    "\n",
    "justification_text = f\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Justification for Raw Ct Analysis:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Due to the observed significant variability of GAPDH expression across the study groups (ANOVA p-value: {pvalue:.3f}, Tukey HSD results showing significant intergroup differences and effect sizes, Supplementary Table S2), and its correlations with disease severity metrics (Supplementary Figure S1), GAPDH was deemed unsuitable as a reference gene for normalization. Using an unstable reference gene would introduce bias into the analysis, potentially confounding the observed differences in miRNA expression. Therefore, subsequent analyses were performed using raw Ct values. Raw Ct analysis provides a more transparent and unbiased approach when a reliable reference gene cannot be identified. This decision aligns with previous research highlighting the potential for reference gene instability in similar contexts, especially inflammatory conditions such as periodontitis (Dheda et al., 2004; Schmittgen and Zakrajsek, 2000; Li et al., 2019; Peng et al., 2012; Ye et al., 2018). While raw Ct analysis relies on the assumption of similar starting RNA amounts across samples, this limitation is mitigated by our quality control measures during sample processing and robust statistical analyses using non-parametric methods. The variability of GAPDH, as evidenced by its significant correlation with bleeding on probing (Supplementary Figure S1 and Supplementary Table S2), further supports the decision to not use it for normalization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Raw Ct analysis offers several advantages in this context. First, it avoids introducing additional bias or assumptions associated with alternative normalization methods. Second, it focuses on identifying miRNAs exhibiting relatively *large* fold-change differences between groups, where smaller variations introduced by a potentially unstable reference gene are less likely to significantly impact the overall conclusions. Third, the use of robust statistical methods, namely ANOVA followed by post hoc tests with Benjamini-Hochberg FDR correction for multiple comparisons and effect size calculations (Cohen's d), ensures rigorous comparisons of raw Ct values between groups. Despite its limitations, the transparency of raw Ct analysis combined with our rigorous statistical approach and the specific context of our study, makes it a suitable and reliable approach for identifying candidate miRNA biomarkers with altered expression in periodontal disease. If suitable alternative reference genes or normalization methods had been identified, these would have been used instead of the raw Ct approach.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "with open(\"results/supplementary/raw_ct_justification.txt\", \"w\") as f:\n",
    "\n",
    "    f.write(justification_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"'GROUP' column is missing from the DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Check if 'GROUP' column exists in both DataFrames\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_raw\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROUP\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_scaled\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGROUP\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column is missing from the DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Define target miRNAs\u001b[39;00m\n\u001b[0;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_mir146a\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_mir146b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_mir381p\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m ]\n",
      "\u001b[1;31mKeyError\u001b[0m: \"'GROUP' column is missing from the DataFrame\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from scipy.stats import shapiro, levene, kruskal, mannwhitneyu\n",
    "import os\n",
    "import scikit_posthocs as sp\n",
    "import pingouin as pg\n",
    "\n",
    "# Load preprocessed data (both raw and scaled training sets)\n",
    "df_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "df_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "\n",
    "# Check if 'GROUP' column exists in both DataFrames\n",
    "if \"GROUP\" not in df_raw.columns or \"GROUP\" not in df_scaled.columns:\n",
    "    raise KeyError(\"'GROUP' column is missing from the DataFrame\")\n",
    "\n",
    "# Define target miRNAs\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "# 1. Non-parametric tests (Raw Ct Data)\n",
    "kruskal_results = {}\n",
    "mannwhitney_results = {}\n",
    "effect_sizes_nonparametric = {}\n",
    "\n",
    "for target in targets:\n",
    "    # Kruskal-Wallis\n",
    "    groups = df_raw[\"GROUP\"].unique()\n",
    "    mirna_data = [df_raw[target][df_raw[\"GROUP\"] == group] for group in groups]\n",
    "    h_statistic, pvalue = stats.kruskal(*mirna_data)\n",
    "    kruskal_results[target] = {\"H-statistic\": h_statistic, \"p-value\": pvalue}\n",
    "\n",
    "    # Post hoc Dunn's test if Kruskal-Wallis is significant\n",
    "    if pvalue < 0.05:\n",
    "        posthoc = sp.posthoc_dunn(\n",
    "            df_raw, val_col=target, group_col=\"GROUP\", p_adjust=\"fdr_bh\"\n",
    "        )\n",
    "        kruskal_results[target][\"Post Hoc p-values\"] = posthoc\n",
    "\n",
    "    # Mann-Whitney U and effect sizes\n",
    "    mannwhitney_results[target] = {}\n",
    "    effect_sizes_nonparametric[target] = {}\n",
    "\n",
    "    for i in range(len(groups)):\n",
    "        for j in range(i + 1, len(groups)):\n",
    "            group1 = df_raw[target][df_raw[\"GROUP\"] == groups[i]]\n",
    "            group2 = df_raw[target][df_raw[\"GROUP\"] == groups[j]]\n",
    "            stat, p = mannwhitneyu(group1, group2)\n",
    "            mannwhitney_results[target][f\"{groups[i]} vs {groups[j]}\"] = (stat, p)\n",
    "\n",
    "            # Calculate effect size (Cliff's delta) using pingouin\n",
    "            effect_size = pg.mwu(group1, group2)[\"CLES\"].values[0]\n",
    "            effect_sizes_nonparametric[target][\n",
    "                f\"{groups[i]} vs {groups[j]}\"\n",
    "            ] = effect_size\n",
    "\n",
    "# Save non-parametric test results\n",
    "nonparametric_results_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (target, test): kruskal_results[target][test]\n",
    "        for target in kruskal_results\n",
    "        for test in kruskal_results[target]\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "mwu_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (miRNA, comparison): mannwhitney_results[miRNA][comparison]\n",
    "        for miRNA in mannwhitney_results\n",
    "        for comparison in mannwhitney_results[miRNA]\n",
    "    },\n",
    "    orient=\"index\",\n",
    "    columns=[\"MWU-statistic\", \"p-value\"],\n",
    ").reset_index(level=1)\n",
    "\n",
    "mwu_df = pd.merge(\n",
    "    mwu_df,\n",
    "    pd.DataFrame(effect_sizes_nonparametric).stack().reset_index(),\n",
    "    left_on=[\"level_1\"],\n",
    "    right_on=[\"level_1\"],\n",
    ")\n",
    "mwu_df.columns = [\"Comparison\", \"MWU-statistic\", \"p-value\", \"miRNA\", \"Effect Size\"]\n",
    "\n",
    "mwu_df.to_csv(\"results/supplementary/mannwhitney_results.csv\")\n",
    "nonparametric_results_df.to_csv(\n",
    "    \"results/supplementary/nonparametric_test_results.csv\", index=True\n",
    ")\n",
    "\n",
    "# 2. Parametric tests (Scaled Ct Data)\n",
    "anova_results = {}\n",
    "tukey_results = {}\n",
    "effect_sizes_parametric = {}\n",
    "\n",
    "for target in targets:\n",
    "    # ANOVA\n",
    "    groups = df_scaled[\"GROUP\"].unique()\n",
    "    mirna_data_scaled = [\n",
    "        df_scaled[target][df_scaled[\"GROUP\"] == group] for group in groups\n",
    "    ]\n",
    "\n",
    "    # Normality check using Shapiro-Wilk test\n",
    "    normality_pvalues = [shapiro(data)[1] for data in mirna_data_scaled]\n",
    "\n",
    "    # Homogeneity of variance check using Levene's test\n",
    "    levene_pvalue = levene(*mirna_data_scaled)[1]\n",
    "\n",
    "    fvalue, pvalue = stats.f_oneway(*mirna_data_scaled)\n",
    "    anova_results[target] = {\"F-statistic\": fvalue, \"p-value\": pvalue}\n",
    "\n",
    "    # Post hoc Tukey HSD if ANOVA is significant\n",
    "    if pvalue < 0.05:\n",
    "        tukey_result = pairwise_tukeyhsd(\n",
    "            df_scaled[target], df_scaled[\"GROUP\"], alpha=0.05\n",
    "        )\n",
    "        # Adjust p-values using Benjamini-Hochberg\n",
    "        reject, pvals_corrected, _, _ = multipletests(\n",
    "            tukey_result.pvalues, method=\"fdr_bh\"\n",
    "        )\n",
    "        anova_results[target][\"Post Hoc p-values\"] = pd.DataFrame(\n",
    "            tukey_result._results_table.data[1:],\n",
    "            columns=tukey_result._results_table.data[0],\n",
    "        )\n",
    "        anova_results[target][\"Post Hoc p-values\"][\"p-adj\"] = pvals_corrected\n",
    "\n",
    "        # Effect sizes (Cohen's d)\n",
    "        cohen_d = {}\n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i + 1, len(groups)):\n",
    "                group1 = mirna_data_scaled[i]\n",
    "                group2 = mirna_data_scaled[j]\n",
    "                d = pg.compute_effsize(group1, group2, eftype=\"cohen\")\n",
    "                cohen_d[f\"{groups[i]} vs {groups[j]}\"] = d\n",
    "        effect_sizes_parametric[target] = cohen_d\n",
    "\n",
    "parametric_results_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (target, test): anova_results[target][test]\n",
    "        for target in anova_results\n",
    "        for test in anova_results[target]\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "parametric_results_df.to_csv(\"results/main/parametric_test_results.csv\")\n",
    "\n",
    "# Combine into a single dataframe for the manuscript\n",
    "parametric_nonparametric_combined = pd.DataFrame()\n",
    "\n",
    "temp_kruskal = pd.DataFrame(kruskal_results).transpose()\n",
    "temp_kruskal[\"miRNA\"] = temp_kruskal.index\n",
    "\n",
    "temp_mwu = mwu_df.copy()\n",
    "temp_anova = parametric_results_df.copy().reset_index().drop(\"level_1\", axis=1)\n",
    "temp_anova[\"miRNA\"] = temp_anova[\"level_0\"]\n",
    "temp_anova = temp_anova.set_index(\"miRNA\")\n",
    "\n",
    "temp_parametric = pd.DataFrame(effect_sizes_parametric).transpose()\n",
    "temp_parametric[\"miRNA\"] = temp_parametric.index\n",
    "\n",
    "parametric_nonparametric_combined = (\n",
    "    temp_kruskal.merge(temp_anova, left_on=\"miRNA\", right_on=\"miRNA\")\n",
    "    .merge(temp_parametric, left_on=\"miRNA\", right_on=\"miRNA\")\n",
    "    .drop(\"level_0\", axis=1)\n",
    ")\n",
    "parametric_nonparametric_combined.to_csv(\n",
    "    \"results/main/parametric_nonparametric_tests.csv\"\n",
    ")\n",
    "\n",
    "# 3. Compare and discuss results (this will be done after all analyses are completed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (both raw and scaled training sets)\n",
    "df_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "df_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "\n",
    "# Define target miRNAs and clinical parameters\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "    \"mean_GAPDH\",\n",
    "]\n",
    "clinical_params = [\n",
    "    \"AGE\",\n",
    "    \"plaque_index\",\n",
    "    \"gingival_index\",\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "    \"number_of_missing_teeth\",\n",
    "]\n",
    "\n",
    "# 1. Correlation analysis (Raw Data)\n",
    "correlation_matrix_raw = df_raw[targets + clinical_params].corr(method=\"pearson\")\n",
    "\n",
    "# Create and save correlation heatmap (raw)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix_raw, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Raw Ct Values)\")\n",
    "plt.savefig(\n",
    "    \"results/supplementary/correlation_heatmap_raw.png\"\n",
    ")  # Save to supplementary\n",
    "plt.close()\n",
    "\n",
    "# 2. Correlation analysis (Scaled Data)\n",
    "correlation_matrix_scaled = df_scaled[targets + clinical_params].corr(method=\"pearson\")\n",
    "\n",
    "# Create and save correlation heatmap (scaled)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix_scaled, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix Heatmap (Robustly Scaled Ct Values)\")\n",
    "plt.savefig(\"results/main/correlation_heatmap_scaled.png\")  # Save to main results\n",
    "plt.close()\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "os.makedirs(\"results/supplementary\", exist_ok=True)\n",
    "\n",
    "# Save correlation matrices\n",
    "correlation_matrix_raw.to_csv(\n",
    "    \"results/supplementary/correlation_matrix_raw.csv\", index=True\n",
    ")\n",
    "correlation_matrix_scaled.to_csv(\n",
    "    \"results/main/correlation_matrix_scaled.csv\", index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (raw and scaled training/test sets)\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "\n",
    "# Define target miRNAs and groups\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "groups = [\"S\", \"G\", \"P\"]\n",
    "\n",
    "\n",
    "# ROC analysis function (with precision-recall)\n",
    "def roc_analysis(X, y, target_name, comparison_name, data_type):\n",
    "    # Split data ONLY if raw data is being used (since scaled data is already split into training and test sets for comparison)\n",
    "    if data_type == \"raw\":\n",
    "        X_train, X_test, y_train_temp, y_test_temp = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test, y_train_temp, y_test_temp = (\n",
    "            X,\n",
    "            y,\n",
    "            y_train_numeric,\n",
    "            y_test_numeric,\n",
    "        )\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\")  # Add solver for better convergence.\n",
    "    model.fit(X_train, y_train_temp)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_temp, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Youden index for optimal cutoff\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    sensitivity = tpr[optimal_idx]\n",
    "    specificity = 1 - fpr[optimal_idx]\n",
    "\n",
    "    try:  # Try calculating accuracy; if it fails, set it to np.nan\n",
    "        accuracy = sum(y_test_temp == (y_prob > optimal_threshold)) / len(\n",
    "            y_test_temp\n",
    "        )  # Using optimal threshold for classification here\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Error calculating accuracy for {target_name} ({comparison_name}, {data_type}): {e}\"\n",
    "        )\n",
    "        accuracy = np.nan\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_test_temp, y_prob)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"{target_name} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve: {comparison_name} ({data_type})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(\n",
    "        f'results/{data_type}/{comparison_name.lower().replace(\" \", \"_\")}',\n",
    "        exist_ok=True,\n",
    "    )\n",
    "\n",
    "    plt.savefig(\n",
    "        f'results/{data_type}/{comparison_name.lower().replace(\" \", \"_\")}/roc_curve_{target_name}.png'\n",
    "    )\n",
    "    plt.close()\n",
    "    return {\n",
    "        \"AUC\": roc_auc,\n",
    "        \"Optimal Cutoff\": optimal_threshold,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"PR AUC\": pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "# 1. Refined ROC Analysis (Control vs. Periodontitis - Addressing Class Imbalance, if applicable)\n",
    "# Since we earlier determined no imbalance, this step isn't needed but is kept as is.  If there were imbalance, you would use the commented out part to handle it.\n",
    "X_raw_cp = X_train_raw[y_train[\"GROUP\"].isin([\"S\", \"P\"])]\n",
    "X_scaled_cp = X_train_scaled[y_train[\"GROUP\"].isin([\"S\", \"P\"])]\n",
    "\n",
    "y_cp = y_train[y_train[\"GROUP\"].isin([\"S\", \"P\"])]\n",
    "# control_count = np.sum(y_cp == 'S')\n",
    "# periodontitis_count = np.sum(y_cp == 'P')\n",
    "\n",
    "# if control_count != periodontitis_count:\n",
    "# Handle class imbalance (e.g., using SMOTE if applicable)\n",
    "#    sm = SMOTE(random_state=42)\n",
    "#    X_raw_resampled, y_resampled = sm.fit_resample(X_raw_cp, y_cp)\n",
    "#    X_scaled_resampled, _ = sm.fit_resample(X_scaled_cp, y_cp)\n",
    "#\n",
    "#    y_cp = y_resampled\n",
    "#    X_raw_cp = pd.DataFrame(X_raw_resampled, columns=targets) # Scaling after resampling\n",
    "#    X_scaled_cp = pd.DataFrame(X_scaled_resampled, columns=targets)\n",
    "# else:\n",
    "#    print(\"No class imbalance detected for Control vs. Periodontitis.\")\n",
    "#    X_raw_resampled, X_scaled_resampled, y_resampled = X_raw_cp, X_scaled_cp, y_cp\n",
    "\n",
    "roc_results_raw_cp = {}\n",
    "roc_results_scaled_cp = {}\n",
    "print(\"Starting ROC analysis\")\n",
    "for target in targets:\n",
    "    roc_results_raw_cp[target] = roc_analysis(\n",
    "        X_raw_cp[[target]], y_cp, target, \"Control vs. Periodontitis\", \"raw\"\n",
    "    )  # Correctly applied to the dataset\n",
    "    roc_results_scaled_cp[target] = roc_analysis(\n",
    "        X_scaled_cp[[target]], y_cp, target, \"Control vs. Periodontitis\", \"scaled\"\n",
    "    )  # Correctly applied to the dataset\n",
    "\n",
    "\n",
    "# Create and save ROC results DataFrames. This will also be used for reporting in the manuscript.\n",
    "pd.DataFrame(roc_results_raw_cp).transpose().to_csv(\n",
    "    \"results/supplementary/roc_results_raw_cp.csv\", index=True\n",
    ")\n",
    "pd.DataFrame(roc_results_scaled_cp).transpose().to_csv(\n",
    "    \"results/main/roc_results_scaled_cp.csv\", index=True\n",
    ")\n",
    "\n",
    "# 2. ROC with Top miRNAs (All Comparisons) and 3. ROC Analysis (All miRNAs, All Comparisons)\n",
    "# will be in the next response due to character limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (Previous code for ROC analysis - Part 1) ...\n",
    "\n",
    "# 2 & 3. ROC with Top miRNAs (All Comparisons) and All miRNAs (All Comparisons)\n",
    "\n",
    "# ROC analysis function (same as before, no changes needed)\n",
    "\n",
    "# Prepare data for all pairwise comparisons (raw and scaled)\n",
    "roc_results_raw = {}\n",
    "roc_results_scaled = {}\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][\n",
    "            targets\n",
    "        ].values\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            targets\n",
    "        ].values\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            targets\n",
    "        ].values\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][targets].values\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        roc_results_raw[comparison_name] = {}\n",
    "        roc_results_scaled[comparison_name] = {}\n",
    "\n",
    "        for k, target in enumerate(targets):\n",
    "            # For correct ROC analysis with train and test datasets\n",
    "            roc_results_raw[comparison_name][target] = roc_analysis(\n",
    "                X_train_raw_subset[:, k].reshape(-1, 1),\n",
    "                X_test_raw_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"raw\",\n",
    "            )\n",
    "            roc_results_scaled[comparison_name][target] = roc_analysis(\n",
    "                X_train_scaled_subset[:, k].reshape(-1, 1),\n",
    "                X_test_scaled_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"scaled\",\n",
    "            )\n",
    "\n",
    "# Top miRNAs ROC analysis (all comparisons)\n",
    "top_mirnas = [\"mean_mir146b\", \"mean_mir155\", \"mean_mir203\"]\n",
    "roc_results_top_mirnas_raw = {}\n",
    "roc_results_top_mirnas_scaled = {}\n",
    "\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ].values\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][top_mirnas].values\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        roc_results_top_mirnas_raw[comparison_name] = {}\n",
    "        roc_results_top_mirnas_scaled[comparison_name] = {}\n",
    "\n",
    "        for k, target in enumerate(top_mirnas):\n",
    "            roc_results_top_mirnas_raw[comparison_name][target] = roc_analysis(\n",
    "                X_train_raw_subset[:, k].reshape(-1, 1),\n",
    "                X_test_raw_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"raw\",\n",
    "            )  # added data_type\n",
    "            roc_results_scaled[comparison_name][target] = roc_analysis(\n",
    "                X_train_scaled_subset[:, k].reshape(-1, 1),\n",
    "                X_test_scaled_subset[:, k].reshape(-1, 1),\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                target,\n",
    "                comparison_name,\n",
    "                \"scaled\",\n",
    "            )  # added data_type\n",
    "\n",
    "# Save ROC results\n",
    "# ... (saving of ROC results will be done after combining miRNAs for ROC is calculated, for organization).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (previous ROC analysis code) ...\n",
    "\n",
    "# Combining Top miRNAs for ROC Analysis\n",
    "\n",
    "roc_results_combined_raw = {}\n",
    "roc_results_combined_scaled = {}\n",
    "roc_results_combined_raw_then_scaled = {}  # For raw combined, then scaled\n",
    "roc_results_combined_scaled_then_combined = {}  # For scaled combined, then combined\n",
    "\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison_name = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # Subset data for current comparison\n",
    "        group1_indices = y_train[\"GROUP\"] == groups[i]\n",
    "        group2_indices = y_train[\"GROUP\"] == groups[j]\n",
    "\n",
    "        X_train_raw_subset = X_train_raw[group1_indices | group2_indices][top_mirnas]\n",
    "        X_test_raw_subset = X_test_raw[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            top_mirnas\n",
    "        ]\n",
    "\n",
    "        X_train_scaled_subset = X_train_scaled[group1_indices | group2_indices][\n",
    "            top_mirnas\n",
    "        ]\n",
    "        X_test_scaled_subset = X_test_scaled[\n",
    "            y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][top_mirnas]\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[group1_indices | group2_indices][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Combine raw Ct values, then scale\n",
    "        X_train_raw_combined = X_train_raw_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "        X_test_raw_combined = X_test_raw_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "        X_train_raw_combined_scaled = scaler.fit_transform(X_train_raw_combined)\n",
    "        X_test_raw_combined_scaled = scaler.transform(X_test_raw_combined)\n",
    "\n",
    "        # Combine scaled Ct values\n",
    "        X_train_scaled_combined = X_train_scaled_subset.mean(axis=1).values.reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        X_test_scaled_combined = X_test_scaled_subset.mean(axis=1).values.reshape(-1, 1)\n",
    "\n",
    "        roc_results_combined_raw[comparison_name] = roc_analysis(\n",
    "            X_train_raw_combined,\n",
    "            X_test_raw_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_raw\",\n",
    "            comparison_name,\n",
    "            \"raw\",\n",
    "        )\n",
    "        roc_results_combined_scaled[comparison_name] = roc_analysis(\n",
    "            X_train_scaled_combined,\n",
    "            X_test_scaled_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_scaled\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "        roc_results_combined_raw_then_scaled[comparison_name] = roc_analysis(\n",
    "            X_train_raw_combined_scaled,\n",
    "            X_test_raw_combined_scaled,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_raw_scaled\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "        roc_results_combined_scaled_then_combined[comparison_name] = roc_analysis(\n",
    "            X_train_scaled_combined,\n",
    "            X_test_scaled_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_scaled_avg\",\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "\n",
    "# Saving ROC Results (all combined in a dictionary):\n",
    "\n",
    "roc_results_all = {\n",
    "    \"raw\": roc_results_raw,\n",
    "    \"scaled\": roc_results_scaled,\n",
    "    \"combined_raw\": roc_results_combined_raw,\n",
    "    \"combined_scaled\": roc_results_combined_scaled,\n",
    "    \"top_mirnas_raw\": roc_results_top_mirnas_raw,\n",
    "    \"top_mirnas_scaled\": roc_results_top_mirnas_scaled,\n",
    "    \"combined_raw_then_scaled\": roc_results_combined_raw_then_scaled,\n",
    "    \"combined_scaled_then_combined\": roc_results_combined_scaled_then_combined,\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate over each data type and create tables\n",
    "for data_type, results in roc_results_all.items():\n",
    "    for comparison, values in results.items():\n",
    "        # Create directory for the ROC results if it doesn't exist\n",
    "        os.makedirs(\n",
    "            f\"results/{'main' if 'scaled' in data_type or 'combined' in data_type else 'supplementary'}/roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        temp = pd.DataFrame(values).transpose()\n",
    "        temp[\"miRNA\"] = temp.index\n",
    "        temp.to_csv(\n",
    "            f\"results/{'main' if 'scaled' in data_type or 'combined' in data_type else 'supplementary'}/roc_{comparison.lower().replace(' ', '_')}_{data_type}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "import json\n",
    "\n",
    "# Save the entire dictionary to a JSON file\n",
    "with open(\"results/main/roc_results_all.json\", \"w\") as fp:\n",
    "    json.dump(roc_results_all, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (scaled training set)\n",
    "df_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "\n",
    "\n",
    "# 1. PCA (All Components)\n",
    "pca = PCA()  # Keep all components\n",
    "pca_result = pca.fit_transform(df_scaled[targets])\n",
    "explained_variance_pca = pca.explained_variance_ratio_\n",
    "\n",
    "# Pairwise 2D scatter plots of PCA components\n",
    "n_components = pca_result.shape[1]\n",
    "for i in range(n_components):\n",
    "    for j in range(i + 1, n_components):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            x=pca_result[:, i],\n",
    "            y=pca_result[:, j],\n",
    "            hue=df_scaled[\"GROUP\"],\n",
    "            palette=\"viridis\",\n",
    "        )\n",
    "        plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "        plt.xlabel(f\"Principal Component {i+1}\")\n",
    "        plt.ylabel(f\"Principal Component {j+1}\")\n",
    "        plt.savefig(f\"results/supplementary/pca_scatter_pc{i+1}_vs_pc{j+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# 3D scatter plot of the first three components\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "ax.scatter(\n",
    "    pca_result[:, 0],\n",
    "    pca_result[:, 1],\n",
    "    pca_result[:, 2],\n",
    "    c=df_scaled[\"GROUP\"].astype(\"category\").cat.codes,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "ax.set_title(\"3D PCA Scatter Plot (PC1 vs PC2 vs PC3)\")\n",
    "ax.set_xlabel(\"Principal Component 1\")\n",
    "ax.set_ylabel(\"Principal Component 2\")\n",
    "ax.set_zlabel(\"Principal Component 3\")\n",
    "plt.savefig(\"results/supplementary/pca_scatter_3d.png\")\n",
    "plt.close()\n",
    "\n",
    "# Save explained variance ratios\n",
    "explained_variance_df = pd.DataFrame(\n",
    "    {\n",
    "        \"PC\": [f\"PC{i+1}\" for i in range(len(explained_variance_pca))],\n",
    "        \"Explained Variance Ratio\": explained_variance_pca,\n",
    "    }\n",
    ")\n",
    "explained_variance_df.to_csv(\n",
    "    \"results/supplementary/pca_explained_variance.csv\", index=False\n",
    ")\n",
    "\n",
    "# 2. LDA (All Components)\n",
    "lda = LDA()  # Retaining all components\n",
    "lda_result = lda.fit_transform(df_scaled[targets], df_scaled[\"GROUP\"])\n",
    "explained_variance_lda = lda.explained_variance_ratio_\n",
    "\n",
    "\n",
    "n_components_lda = lda_result.shape[1]\n",
    "\n",
    "for i in range(n_components_lda):\n",
    "    for j in range(i + 1, n_components_lda):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            x=lda_result[:, i],\n",
    "            y=lda_result[:, j],\n",
    "            hue=df_scaled[\"GROUP\"],\n",
    "            palette=\"viridis\",\n",
    "        )\n",
    "        plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "        plt.xlabel(f\"Linear Discriminant {i+1}\")\n",
    "        plt.ylabel(f\"Linear Discriminant {j+1}\")\n",
    "        plt.savefig(f\"results/supplementary/lda_scatter_ld{i+1}_vs_ld{j+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Save explained variance ratios\n",
    "explained_variance_lda_df = pd.DataFrame(\n",
    "    {\n",
    "        \"LD\": [f\"LD{i+1}\" for i in range(len(explained_variance_lda))],\n",
    "        \"Explained Variance Ratio\": explained_variance_lda,\n",
    "    }\n",
    ")\n",
    "explained_variance_lda_df.to_csv(\n",
    "    \"results/supplementary/lda_explained_variance.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (scaled training and test sets)\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert GROUP column to numerical using cat.codes\n",
    "y_train_numeric = y_train[\"GROUP\"].astype(\"category\").cat.codes\n",
    "y_test_numeric = y_test[\"GROUP\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Define target miRNAs (features)\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "X_train = X_train_scaled[targets]\n",
    "X_test = X_test_scaled[targets]\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        max_iter=1000, solver=\"liblinear\"\n",
    "    ),  # Increased max_iter and added solver for better convergence\n",
    "    \"LDA\": LDA(),\n",
    "    \"SVM\": SVC(\n",
    "        probability=True\n",
    "    ),  # Add probability parameter so that roc_auc_score can be calculated\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"Neural Network\": MLPClassifier(\n",
    "        max_iter=1000, random_state=42, early_stopping=True\n",
    "    ),  # Increased max_iter and add early stopping to avoid overfitting\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train_numeric)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = (\n",
    "        clf.predict_proba(X_test)\n",
    "        if hasattr(clf, \"predict_proba\")\n",
    "        else clf.decision_function(X_test)\n",
    "    )  # Get predicted probabilities or decision function if probabilities are not available\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\": accuracy_score(y_test_numeric, y_pred),\n",
    "        \"precision\": precision_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "        \"f1_score\": f1_score(y_test_numeric, y_pred, average=\"weighted\"),\n",
    "    }\n",
    "    # Compute roc_auc_score only for binary and multiclass problems\n",
    "    if len(set(y_test_numeric)) <= 2:\n",
    "        try:\n",
    "            results[name][\"auc\"] = roc_auc_score(y_test_numeric, y_prob)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                \"Only one class present in y_true. ROC AUC score is not defined in that case. Skipping AUC calculation.\"\n",
    "            )\n",
    "    elif len(set(y_test_numeric)) > 2:\n",
    "        results[name][\"auc\"] = roc_auc_score(y_test_numeric, y_prob, multi_class=\"ovr\")\n",
    "\n",
    "    # Add classification report to show precision, recall and f1-score for each class\n",
    "    results[name][\"classification_report\"] = classification_report(\n",
    "        y_test_numeric, y_pred, output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Add confusion matrix as well\n",
    "    cm = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        cmap=\"Blues\",\n",
    "        fmt=\"g\",\n",
    "        xticklabels=sorted(y.unique()),\n",
    "        yticklabels=sorted(y.unique()),\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix: {name}\")\n",
    "    plt.savefig(f\"results/main/{name}_cm.png\")\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importance = classifiers[\"Random Forest\"].feature_importances_\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": targets, \"Importance\": feature_importance}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(\"results/main\", exist_ok=True)\n",
    "\n",
    "# Convert the dictionaries to DataFrames and save the results\n",
    "pd.DataFrame(results).transpose().to_csv(\n",
    "    \"results/main/classification_results.csv\", index=True\n",
    ")\n",
    "\n",
    "feature_importance_df.to_csv(\"results/main/feature_importance.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Load preprocessed data (raw and scaled training/test sets)\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "# Define target miRNAs and groups\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "top_targets = [\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "]  # Based on prior feature selection\n",
    "groups = [\"S\", \"G\", \"P\"]\n",
    "\n",
    "\n",
    "# ROC analysis function (same as before)\n",
    "# ... (include the roc_analysis function from the previous code block) ...\n",
    "\n",
    "\n",
    "# 1. Combined miRNA Scores (using training data only for proper assessment)\n",
    "X_train_combined = X_train_scaled.copy()\n",
    "X_test_combined = X_test_scaled.copy()\n",
    "\n",
    "# Simple Average\n",
    "X_train_combined[\"combined_avg\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "X_test_combined[\"combined_avg\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "# Weighted Average (using feature importances from Random Forest)\n",
    "with open(\"src/scaler.pkl\", \"rb\") as f:\n",
    "    scaler_loaded = pickle.load(f)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_scaled[targets], y_train_numeric)  # Using y_train_numeric here\n",
    "feature_importances = rf.feature_importances_\n",
    "top_mirna_indices = [targets.index(mirna) for mirna in top_targets]\n",
    "top_mirna_importances = feature_importances[top_mirna_indices]\n",
    "\n",
    "X_train_combined[\"combined_weighted\"] = np.dot(\n",
    "    X_train_scaled[top_targets], top_mirna_importances / np.sum(top_mirna_importances)\n",
    ")\n",
    "X_test_combined[\"combined_weighted\"] = np.dot(\n",
    "    X_test_scaled[top_targets], top_mirna_importances / np.sum(top_mirna_importances)\n",
    ")\n",
    "\n",
    "# PCA (First Principal Component)\n",
    "pca = PCA(n_components=1)\n",
    "X_train_combined[\"combined_pca\"] = pca.fit_transform(X_train_scaled[top_targets])\n",
    "X_test_combined[\"combined_pca\"] = pca.transform(X_test_scaled[top_targets])\n",
    "\n",
    "# LDA (First Linear Discriminant)\n",
    "lda = LDA(n_components=1)\n",
    "X_train_combined[\"combined_lda\"] = lda.fit_transform(\n",
    "    X_train_scaled[top_targets], y_train.values.ravel()\n",
    ")  # Needs numerical target values for training\n",
    "X_test_combined[\"combined_lda\"] = lda.transform(X_test_scaled[top_targets])\n",
    "\n",
    "combined_methods = [\"combined_avg\", \"combined_weighted\", \"combined_pca\", \"combined_lda\"]\n",
    "\n",
    "# 2. ROC Analysis with Combined Scores (Raw and Scaled, All Comparisons)\n",
    "# Initialize dictionaries to store ROC results\n",
    "roc_results_combined_raw = {}\n",
    "roc_results_combined_scaled = {}\n",
    "\n",
    "for i in range(len(groups)):\n",
    "    for j in range(i + 1, len(groups)):\n",
    "        comparison = f\"{groups[i]} vs {groups[j]}\"\n",
    "\n",
    "        # ROC for Averaging Raw Values and Then Scaling as done earlier.\n",
    "        # This is done before to show that even if averaging is done using raw values, robust normalization still enhances the AUC, however marginally.\n",
    "        temp_X_raw = X_train_raw[targets].copy()\n",
    "        temp_X_test = X_test_raw[targets].copy()\n",
    "\n",
    "        temp_X_raw[\"combined_top_raw\"] = temp_X_raw[top_targets].mean(axis=1)\n",
    "        temp_X_test[\"combined_top_raw\"] = temp_X_test[top_targets].mean(axis=1)\n",
    "\n",
    "        # Subset the combined data, scale and perform ROC using the scaled data.\n",
    "        combined_X_train_raw = temp_X_raw[\n",
    "            y_train[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "        ][\"combined_top_raw\"].values.reshape(-1, 1)\n",
    "        combined_X_test_raw = temp_X_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\n",
    "            \"combined_top_raw\"\n",
    "        ].values.reshape(-1, 1)\n",
    "\n",
    "        combined_X_train_raw_scaled = scaler_loaded.fit_transform(combined_X_train_raw)\n",
    "        combined_X_test_raw_scaled = scaler_loaded.transform(combined_X_test_raw)\n",
    "\n",
    "        y_train_subset = (\n",
    "            y_train[y_train[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "        y_test_subset = (\n",
    "            y_test[y_test[\"GROUP\"].isin([groups[i], groups[j]])][\"GROUP\"]\n",
    "            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # Initialize the result dictionaries for the current comparison.\n",
    "        roc_results_combined_raw[comparison] = {}\n",
    "        roc_results_combined_scaled[comparison] = {}\n",
    "\n",
    "        roc_results_combined_raw[comparison][\"combined_top_raw_scaled\"] = roc_analysis(\n",
    "            combined_X_train_raw_scaled,\n",
    "            combined_X_test_raw_scaled,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            \"combined_top_raw_scaled\",\n",
    "            comparison,\n",
    "            \"scaled\",\n",
    "        )\n",
    "\n",
    "        for method in combined_methods:\n",
    "            # Subset data for current comparison\n",
    "            current_X_train = X_train_combined[\n",
    "                y_train[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "            ][method].values.reshape(-1, 1)\n",
    "            current_X_test = X_test_combined[\n",
    "                y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "            ][method].values.reshape(-1, 1)\n",
    "\n",
    "            roc_results_combined_scaled[comparison][method] = roc_analysis(\n",
    "                current_X_train,\n",
    "                current_X_test,\n",
    "                y_train_subset,\n",
    "                y_test_subset,\n",
    "                method,\n",
    "                comparison,\n",
    "                \"scaled\",\n",
    "            )\n",
    "\n",
    "# Saving ROC Results (all methods together):\n",
    "roc_results_combined_all = {\n",
    "    \"combined_raw\": roc_results_combined_raw,\n",
    "    \"combined_scaled\": roc_results_combined_scaled,\n",
    "}\n",
    "\n",
    "# Iterate over each data type and create tables\n",
    "for data_type, results in roc_results_combined_all.items():\n",
    "    for comparison, values in results.items():\n",
    "        # Create directory for the ROC results if it doesn't exist\n",
    "        os.makedirs(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        temp = pd.DataFrame(values).transpose()\n",
    "        temp[\"method\"] = temp.index\n",
    "        temp.to_csv(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "# Save the entire dictionary to a JSON file\n",
    "import json\n",
    "\n",
    "with open(\"results/main/roc_results_combined_all.json\", \"w\") as fp:\n",
    "    json.dump(roc_results_combined_all, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (scaled training and testing sets)\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "\n",
    "    # Define target miRNAs (features)\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "    X_train = X_train_scaled[targets]\n",
    "    X_test = X_test_scaled[targets]\n",
    "\n",
    "    # 1. PCA (All Components)\n",
    "    pca = PCA()  # Keep all components\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    explained_variance_pca = pca.explained_variance_ratio_\n",
    "\n",
    "    # Pairwise 2D scatter plots of PCA components (on training data)\n",
    "    n_components = X_train_pca.shape[1]\n",
    "    for i in range(n_components):\n",
    "        for j in range(i + 1, n_components):\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.scatterplot(\n",
    "                x=X_train_pca[:, i],\n",
    "                y=X_train_pca[:, j],\n",
    "                hue=y_train[\"GROUP\"],\n",
    "                palette=\"viridis\",\n",
    "            )\n",
    "            plt.title(f\"PCA Scatter Plot (PC{i+1} vs PC{j+1})\")\n",
    "            plt.xlabel(f\"Principal Component {i+1}\")\n",
    "            plt.ylabel(f\"Principal Component {j+1}\")\n",
    "            plt.savefig(f\"results/supplementary/pca_scatter_pc{i+1}_vs_pc{j+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # 3D scatter plot of the first three components (on training data)\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111, projection=\"3d\")\n",
    "            ax.scatter(\n",
    "                X_train_pca[:, 0],\n",
    "                X_train_pca[:, 1],\n",
    "                X_train_pca[:, 2],\n",
    "                c=y_train[\"GROUP\"].astype(\"category\").cat.codes,\n",
    "                cmap=\"viridis\",\n",
    "            )\n",
    "            ax.set_title(\"3D PCA Scatter Plot (PC1 vs PC2 vs PC3)\")\n",
    "            ax.set_xlabel(\"Principal Component 1\")\n",
    "            ax.set_ylabel(\"Principal Component 2\")\n",
    "            ax.set_zlabel(\"Principal Component 3\")\n",
    "            plt.savefig(\"results/supplementary/pca_scatter_3d.png\")\n",
    "            plt.close()\n",
    "\n",
    "            # Save explained variance ratios\n",
    "            explained_variance_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"PC\": [f\"PC{i+1}\" for i in range(len(explained_variance_pca))],\n",
    "                    \"Explained Variance Ratio\": explained_variance_pca,\n",
    "                }\n",
    "            )\n",
    "            # Create directories if they don't exist\n",
    "            os.makedirs(\"results/supplementary\", exist_ok=True)\n",
    "\n",
    "            explained_variance_df.to_csv(\n",
    "                \"results/supplementary/pca_explained_variance.csv\", index=False\n",
    "            )\n",
    "\n",
    "            # 2. LDA (All Components)\n",
    "            lda = LDA()  # Retaining all components\n",
    "            X_train_lda = lda.fit_transform(\n",
    "                X_train, y_train_numeric\n",
    "            )  # Correctly use group labels for LDA\n",
    "            X_test_lda = lda.transform(X_test)\n",
    "            explained_variance_lda = lda.explained_variance_ratio_\n",
    "\n",
    "            # Pairwise 2-D scatter plots for LDA\n",
    "            n_components_lda = X_train_lda.shape[1]\n",
    "            for i in range(n_components_lda):\n",
    "                for j in range(i + 1, n_components_lda):\n",
    "                    plt.figure(figsize=(8, 6))\n",
    "                    sns.scatterplot(\n",
    "                        x=X_train_lda[:, i],\n",
    "                        y=X_train_lda[:, j],\n",
    "                        hue=y_train[\"GROUP\"],\n",
    "                        palette=\"viridis\",\n",
    "                    )\n",
    "                    plt.title(f\"LDA Scatter Plot (LD{i+1} vs LD{j+1})\")\n",
    "                    plt.xlabel(f\"Linear Discriminant {i+1}\")\n",
    "                    plt.ylabel(f\"Linear Discriminant {j+1}\")\n",
    "                    plt.savefig(\n",
    "                        f\"results/supplementary/lda_scatter_ld{i+1}_vs_ld{j+1}.png\"\n",
    "                    )\n",
    "                    plt.close()\n",
    "\n",
    "                    # Save explained variance ratios\n",
    "                    explained_variance_lda_df = pd.DataFrame(\n",
    "                        {\n",
    "                            \"LD\": [\n",
    "                                f\"LD{i+1}\" for i in range(len(explained_variance_lda))\n",
    "                            ],\n",
    "                            \"Explained Variance Ratio\": explained_variance_lda,\n",
    "                        }\n",
    "                    )\n",
    "                    explained_variance_lda_df.to_csv(\n",
    "                        \"results/supplementary/lda_explained_variance.csv\", index=False\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Load preprocessed data (scaled training and test sets)\n",
    "X_train = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train['GROUP']):\n",
    "    y_train_numeric = y_train['GROUP'].cat.codes\n",
    "    y_test_numeric = y_test['GROUP'].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train['GROUP']\n",
    "    y_test_numeric = y_test['GROUP']\n",
    "\n",
    "\n",
    "    # Define target miRNAs (features)\n",
    "    targets = ['mean_mir146a', 'mean_mir146b', 'mean_mir155', 'mean_mir203', 'mean_mir223', 'mean_mir381p']\n",
    "\n",
    "    X_train = X_train[targets]\n",
    "    X_test = X_test[targets]\n",
    "\n",
    "\n",
    "    # Initialize classifiers\n",
    "    classifiers = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, solver = 'liblinear'),\n",
    "        'LDA': LDA(),\n",
    "        'SVM': SVC(probability=True),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'Neural Network': MLPClassifier(max_iter=1000, random_state=42, early_stopping = True)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        clf.fit(X_train, y_train_numeric)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_prob = clf.predict_proba(X_test) if hasattr(clf, 'predict_proba') else clf.decision_function(X_test)\n",
    "\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy_score(y_test_numeric, y_pred),\n",
    "            'precision': precision_score(y_test_numeric, y_pred, average='weighted'),\n",
    "            'recall': recall_score(y_test_numeric, y_pred, average='weighted'),\n",
    "            'f1_score': f1_score(y_test_numeric, y_pred, average='weighted')\n",
    "        }\n",
    "        # Compute roc_auc_score only for binary and multiclass problems\n",
    "        if len(set(y_test_numeric)) <= 2:\n",
    "            try:\n",
    "                results[name]['auc'] = roc_auc_score(y_test_numeric, y_prob)\n",
    "            except ValueError:\n",
    "                print(\"Only one class present in y_true. ROC AUC score is not defined in that case. Skipping AUC calculation.\")\n",
    "            elif len(set(y_test_numeric)) > 2:\n",
    "                results[name]['auc'] = roc_auc_score(y_test_numeric, y_prob, multi_class = 'ovr')\n",
    "\n",
    "                # Add classification report and confusion matrix\n",
    "                results[name]['classification_report'] = classification_report(y_test_numeric, y_pred, output_dict=True, zero_division = 0)\n",
    "                cm = confusion_matrix(y_test_numeric, y_pred)\n",
    "\n",
    "                plt.figure(figsize=(8,6))\n",
    "                sns.heatmap(cm, annot=True, cmap = 'Blues', fmt = 'g', xticklabels = sorted(y_test['GROUP'].unique()), yticklabels=sorted(y_test['GROUP'].unique())) # corrected labels\n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('Actual')\n",
    "                plt.title(f'Confusion Matrix: {name}')\n",
    "                plt.savefig(f'results/main/{name}_cm.png')\n",
    "                plt.close()\n",
    "\n",
    "                # Feature importance for Random Forest\n",
    "                feature_importance = classifiers['Random Forest'].feature_importances_\n",
    "                feature_importance_df = pd.DataFrame({'Feature': targets, 'Importance': feature_importance}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "                # Create directories if they don't exist\n",
    "                os.makedirs('results/main', exist_ok=True)\n",
    "\n",
    "                # Convert the dictionaries to DataFrames and save the results\n",
    "                pd.DataFrame(results).transpose().to_csv('results/main/classification_results.csv', index = True)\n",
    "                feature_importance_df.to_csv('results/main/feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load preprocessed data (raw and scaled training/test sets)\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "\n",
    "    # Define target miRNAs and groups\n",
    "    targets = [\n",
    "        \"mean_mir146a\",\n",
    "        \"mean_mir146b\",\n",
    "        \"mean_mir155\",\n",
    "        \"mean_mir203\",\n",
    "        \"mean_mir223\",\n",
    "        \"mean_mir381p\",\n",
    "    ]\n",
    "    top_targets = [\"mean_mir146b\", \"mean_mir155\", \"mean_mir203\"]\n",
    "    groups = [\"S\", \"G\", \"P\"]\n",
    "\n",
    "    # ROC analysis function (same as before)\n",
    "\n",
    "    def roc_analysis(\n",
    "        X_train, X_test, y_train, y_test, target_name, comparison_name, data_type\n",
    "    ):\n",
    "        model = LogisticRegression(solver=\"liblinear\")\n",
    "        model.fit(X_train, y_train)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        optimal_idx = np.argmax(tpr - fpr)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        sensitivity = tpr[optimal_idx]\n",
    "        specificity = 1 - fpr[optimal_idx]\n",
    "        try:\n",
    "            accuracy = sum(y_test == (y_prob > optimal_threshold)) / len(y_test)\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Error calculating accuracy for {target_name} ({comparison_name}, {data_type}): {e}\"\n",
    "            )\n",
    "            accuracy = np.nan\n",
    "\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "\n",
    "            return {\n",
    "                \"AUC\": roc_auc,\n",
    "                \"Optimal Cutoff\": optimal_threshold,\n",
    "                \"Sensitivity\": sensitivity,\n",
    "                \"Specificity\": specificity,\n",
    "                \"Accuracy\": accuracy,\n",
    "                \"PR AUC\": pr_auc,\n",
    "            }\n",
    "\n",
    "            # 1. Combined miRNA Scores (on training data)\n",
    "            combined_methods = [\n",
    "                \"combined_avg\",\n",
    "                \"combined_weighted\",\n",
    "                \"combined_pca\",\n",
    "                \"combined_lda\",\n",
    "            ]\n",
    "\n",
    "            X_train_combined = X_train_scaled.copy()\n",
    "            X_test_combined = X_test_scaled.copy()\n",
    "\n",
    "            # Simple Average\n",
    "            X_train_combined[\"combined_avg\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "            X_test_combined[\"combined_avg\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "            # Weighted Average (using feature importances from Random Forest)\n",
    "            with open(\"src/scaler.pkl\", \"rb\") as f:\n",
    "                scaler_loaded = pickle.load(f)\n",
    "\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                rf.fit(\n",
    "                    X_train_scaled[targets], y_train_numeric\n",
    "                )  # Fit on numerical labels\n",
    "                feature_importances = rf.feature_importances_\n",
    "                top_mirna_indices = [targets.index(mirna) for mirna in top_targets]\n",
    "                top_mirna_importances = feature_importances[top_mirna_indices]\n",
    "\n",
    "                X_train_combined[\"combined_weighted\"] = np.dot(\n",
    "                    X_train_scaled[top_targets],\n",
    "                    top_mirna_importances / np.sum(top_mirna_importances),\n",
    "                )\n",
    "                X_test_combined[\"combined_weighted\"] = np.dot(\n",
    "                    X_test_scaled[top_targets],\n",
    "                    top_mirna_importances / np.sum(top_mirna_importances),\n",
    "                )\n",
    "\n",
    "                # PCA (First Principal Component)\n",
    "                pca = PCA(n_components=1)\n",
    "                X_train_combined[\"combined_pca\"] = pca.fit_transform(\n",
    "                    X_train_scaled[top_targets]\n",
    "                )\n",
    "                X_test_combined[\"combined_pca\"] = pca.transform(\n",
    "                    X_test_scaled[top_targets]\n",
    "                )\n",
    "\n",
    "                # LDA (First Linear Discriminant)\n",
    "                lda = LDA(n_components=1)\n",
    "                X_train_combined[\"combined_lda\"] = lda.fit_transform(\n",
    "                    X_train_scaled[top_targets], y_train_numeric\n",
    "                )  # Fit on numeric labels\n",
    "                X_test_combined[\"combined_lda\"] = lda.transform(\n",
    "                    X_test_scaled[top_targets]\n",
    "                )\n",
    "\n",
    "                # 2. ROC Analysis with Combined Scores (Raw and Scaled, All Comparisons)\n",
    "                roc_results_combined_raw = {}\n",
    "                roc_results_combined_scaled = {}\n",
    "\n",
    "                # Iterate through group comparisons\n",
    "                for i in range(len(groups)):\n",
    "                    for j in range(i + 1, len(groups)):\n",
    "                        comparison = f\"{groups[i]} vs {groups[j]}\"\n",
    "                        roc_results_combined_raw[comparison] = {}\n",
    "                        roc_results_combined_scaled[comparison] = {}\n",
    "                        # Subset data for the comparison, using y_train and y_test values for subsetting\n",
    "                        train_subset = y_train[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "                        test_subset = y_test[\"GROUP\"].isin([groups[i], groups[j]])\n",
    "\n",
    "                        # Raw and scaled data should also be subsetted using both X_train, X_test and also y_train and y_test indices\n",
    "                        X_train_raw_subset = X_train_raw[train_subset]\n",
    "                        X_test_raw_subset = X_test_raw[test_subset]\n",
    "\n",
    "                        X_train_scaled_subset = X_train_scaled[train_subset]\n",
    "                        X_test_scaled_subset = X_test_scaled[test_subset]\n",
    "                        # Convert y_train, and y_test labels to numeric\n",
    "                        y_train_subset = (\n",
    "                            y_train[train_subset][\"GROUP\"]\n",
    "                            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "                            .values\n",
    "                        )\n",
    "                        y_test_subset = (\n",
    "                            y_test[test_subset][\"GROUP\"]\n",
    "                            .apply(lambda x: 0 if x == groups[i] else 1)\n",
    "                            .values\n",
    "                        )\n",
    "\n",
    "                        # Combine raw values, scale and calculate ROC\n",
    "                        combined_X_train_raw = (\n",
    "                            X_train_raw_subset[top_targets]\n",
    "                            .mean(axis=1)\n",
    "                            .values.reshape(-1, 1)\n",
    "                        )\n",
    "                        combined_X_test_raw = (\n",
    "                            X_test_raw_subset[top_targets]\n",
    "                            .mean(axis=1)\n",
    "                            .values.reshape(-1, 1)\n",
    "                        )\n",
    "\n",
    "                        combined_X_train_raw_scaled = scaler.fit_transform(\n",
    "                            combined_X_train_raw\n",
    "                        )\n",
    "                        combined_X_test_raw_scaled = scaler.transform(\n",
    "                            combined_X_test_raw\n",
    "                        )\n",
    "\n",
    "                        roc_results_combined_raw[comparison][\n",
    "                            \"combined_top_raw_scaled\"\n",
    "                        ] = roc_analysis(\n",
    "                            combined_X_train_raw_scaled,\n",
    "                            combined_X_test_raw_scaled,\n",
    "                            y_train_subset,\n",
    "                            y_test_subset,\n",
    "                            \"combined_top_raw_scaled\",\n",
    "                            comparison,\n",
    "                            \"scaled\",\n",
    "                        )\n",
    "\n",
    "                        for method in combined_methods:\n",
    "                            # Subset data for current comparison, using training set for fitting, test set for transforming as for other methods.\n",
    "                            X_train_subset = X_train_combined[train_subset][\n",
    "                                method\n",
    "                            ].values.reshape(-1, 1)\n",
    "                            X_test_subset = X_test_combined[test_subset][\n",
    "                                method\n",
    "                            ].values.reshape(-1, 1)\n",
    "                            roc_results_combined_scaled[comparison][method] = (\n",
    "                                roc_analysis(\n",
    "                                    X_train_subset,\n",
    "                                    X_test_subset,\n",
    "                                    y_train_subset,\n",
    "                                    y_test_subset,\n",
    "                                    method,\n",
    "                                    comparison,\n",
    "                                    \"scaled\",\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                            # Saving ROC Results and Correlations with Clinical Parameters: (Will be in the next response due to character limitations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083b4a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (previous code for combining miRNAs and ROC analysis) ...\n",
    "\n",
    "# Saving ROC Results (all methods together):\n",
    "roc_results_combined_all = {\n",
    "    \"combined_raw\": roc_results_combined_raw,\n",
    "    \"combined_scaled\": roc_results_combined_scaled,\n",
    "}\n",
    "\n",
    "# Iterate over each data type and create tables\n",
    "for data_type, results in roc_results_combined_all.items():\n",
    "    for comparison, values in results.items():\n",
    "        # Create directory for the ROC results if it doesn't exist\n",
    "        os.makedirs(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        temp = pd.DataFrame(values).transpose()\n",
    "        temp[\"method\"] = temp.index\n",
    "        temp.to_csv(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "# Save the entire dictionary to a JSON file\n",
    "import json\n",
    "\n",
    "with open(\"results/main/roc_results_combined_all.json\", \"w\") as fp:\n",
    "    json.dump(roc_results_combined_all, fp, indent=4)\n",
    "\n",
    "# Correlation Analysis for Combined miRNAs (raw and scaled)\n",
    "\n",
    "# Raw combined miRNA data (average of raw Ct values of top 3 miRNAs)\n",
    "X_train_raw[\"combined_raw\"] = X_train_raw[top_targets].mean(axis=1)\n",
    "X_test_raw[\"combined_raw\"] = X_test_raw[top_targets].mean(axis=1)\n",
    "# Robustly scaled combined miRNA data\n",
    "X_train_scaled[\"combined_scaled_top\"] = X_train_scaled[top_targets].mean(axis=1)\n",
    "X_test_scaled[\"combined_scaled_top\"] = X_test_scaled[top_targets].mean(axis=1)\n",
    "\n",
    "# Calculate correlations for combined miRNAs (both raw and scaled) and then with all clinical parameters.\n",
    "correlation_matrix_combined_raw = X_train_raw[[\"combined_raw\"] + clinical_params].corr(\n",
    "    method=\"pearson\"\n",
    ")\n",
    "correlation_matrix_combined_scaled = X_train_scaled[\n",
    "    [\"combined_scaled_top\"] + clinical_params\n",
    "].corr(method=\"pearson\")\n",
    "\n",
    "# Generate and save heatmaps\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix_combined_raw, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Combined Raw Ct Values)\")\n",
    "plt.savefig(\n",
    "    \"results/supplementary/correlation_heatmap_combined_raw.png\"\n",
    ")  # Save to supplementary\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(correlation_matrix_combined_scaled, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap (Combined Scaled Ct Values)\")\n",
    "plt.savefig(\n",
    "    \"results/main/correlation_heatmap_combined_scaled.png\"\n",
    ")  # Save to main results\n",
    "plt.close()\n",
    "\n",
    "# Save correlation matrices\n",
    "correlation_matrix_combined_raw.to_csv(\n",
    "    \"results/supplementary/correlation_matrix_combined_raw.csv\", index=True\n",
    ")\n",
    "correlation_matrix_combined_scaled.to_csv(\n",
    "    \"results/main/correlation_matrix_combined_scaled.csv\", index=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bb9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Load preprocessed data (raw and scaled training/test sets)\n",
    "X_train_raw = pd.read_csv(\"data/processed/X_train_raw.csv\")\n",
    "X_test_raw = pd.read_csv(\"data/processed/X_test_raw.csv\")\n",
    "\n",
    "y_train = pd.read_csv(\"data/processed/y_train.csv\")\n",
    "y_test = pd.read_csv(\"data/processed/y_test.csv\")\n",
    "\n",
    "X_train_scaled = pd.read_csv(\"data/processed/X_train_scaled.csv\")\n",
    "X_test_scaled = pd.read_csv(\"data/processed/X_test_scaled.csv\")\n",
    "\n",
    "# Convert y_train and y_test to numeric if necessary\n",
    "if pd.api.types.is_categorical_dtype(y_train[\"GROUP\"]):\n",
    "    y_train_numeric = y_train[\"GROUP\"].cat.codes\n",
    "    y_test_numeric = y_test[\"GROUP\"].cat.codes\n",
    "else:\n",
    "    y_train_numeric = y_train[\"GROUP\"]\n",
    "    y_test_numeric = y_test[\"GROUP\"]\n",
    "\n",
    "# Define target miRNAs and groups\n",
    "targets = [\n",
    "    \"mean_mir146a\",\n",
    "    \"mean_mir146b\",\n",
    "    \"mean_mir155\",\n",
    "    \"mean_mir203\",\n",
    "    \"mean_mir223\",\n",
    "    \"mean_mir381p\",\n",
    "]\n",
    "groups = [\"S\", \"G\", \"P\"]\n",
    "clinical_params = [\n",
    "    \"AGE\",\n",
    "    \"plaque_index\",\n",
    "    \"gingival_index\",\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "    \"number_of_missing_teeth\",\n",
    "]\n",
    "\n",
    "\n",
    "# ROC analysis function (same as before)\n",
    "# ... (include roc_analysis from previous code blocks) ...\n",
    "\n",
    "# Combine top miRNAs for ROC Analysis, using only the Control vs. the other groups.\n",
    "top_mirnas = [\"mean_mir146b\", \"mean_mir155\", \"mean_mir203\"]\n",
    "combined_methods = [\"combined_avg\", \"combined_weighted\", \"combined_pca\", \"combined_lda\"]\n",
    "\n",
    "roc_results_combined_raw = {}\n",
    "roc_results_combined_scaled = {}\n",
    "\n",
    "# Initialize scaler for re-use\n",
    "scaler = RobustScaler()\n",
    "\n",
    "for i in range(1, len(groups)):  # Starts from 1 since 0 is Control\n",
    "    comparison = f\"S vs {groups[i]}\"\n",
    "    roc_results_combined_raw[comparison] = {}\n",
    "    roc_results_combined_scaled[comparison] = {}\n",
    "    # Subset data for the comparison, using y_train and y_test values for subsetting\n",
    "    train_subset = y_train[\"GROUP\"].isin([\"S\", groups[i]])\n",
    "    test_subset = y_test[\"GROUP\"].isin([\"S\", groups[i]])\n",
    "\n",
    "    X_train_raw_subset = X_train_raw[train_subset][targets]\n",
    "    X_test_raw_subset = X_test_raw[test_subset][targets]\n",
    "\n",
    "    X_train_scaled_subset = X_train_scaled[train_subset][targets]\n",
    "    X_test_scaled_subset = X_test_scaled[test_subset][targets]\n",
    "\n",
    "    # Convert y_train, and y_test labels to numeric\n",
    "    y_train_subset = (\n",
    "        y_train[train_subset][\"GROUP\"].apply(lambda x: 0 if x == \"S\" else 1).values\n",
    "    )\n",
    "    y_test_subset = (\n",
    "        y_test[test_subset][\"GROUP\"].apply(lambda x: 0 if x == \"S\" else 1).values\n",
    "    )\n",
    "\n",
    "    # Combine raw values, scale and calculate ROC\n",
    "    combined_X_train_raw = (\n",
    "        X_train_raw_subset[top_targets].mean(axis=1).values.reshape(-1, 1)\n",
    "    )\n",
    "    combined_X_test_raw = (\n",
    "        X_test_raw_subset[top_targets].mean(axis=1).values.reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    combined_X_train_raw_scaled = scaler.fit_transform(combined_X_train_raw)\n",
    "    combined_X_test_raw_scaled = scaler.transform(combined_X_test_raw)\n",
    "\n",
    "    roc_results_combined_raw[comparison][\"combined_top_raw_scaled\"] = roc_analysis(\n",
    "        combined_X_train_raw_scaled,\n",
    "        combined_X_test_raw_scaled,\n",
    "        y_train_subset,\n",
    "        y_test_subset,\n",
    "        \"combined_top_raw_scaled\",\n",
    "        comparison,\n",
    "        \"scaled\",\n",
    "    )\n",
    "\n",
    "    # Perform simple averaging of scaled values and store for later ROC analysis.\n",
    "    X_train_combined = {}\n",
    "    X_test_combined = {}\n",
    "    X_train_combined[\"combined_avg\"] = X_train_scaled_subset[top_targets].mean(axis=1)\n",
    "    X_test_combined[\"combined_avg\"] = X_test_scaled_subset[top_targets].mean(axis=1)\n",
    "\n",
    "    # Weighted average (training data) using raw and scaled values.\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_scaled_subset[targets], y_train_subset)  # Fit using training set.\n",
    "    feature_importances = rf.feature_importances_\n",
    "    top_mirna_indices = [targets.index(mirna) for mirna in top_targets]\n",
    "    top_mirna_importances = feature_importances[top_mirna_indices]\n",
    "    X_train_combined[\"combined_weighted\"] = np.dot(\n",
    "        X_train_scaled_subset[top_targets],\n",
    "        top_mirna_importances / np.sum(top_mirna_importances),\n",
    "    )\n",
    "    X_test_combined[\"combined_weighted\"] = np.dot(\n",
    "        X_test_scaled_subset[top_targets],\n",
    "        top_mirna_importances / np.sum(top_mirna_importances),\n",
    "    )\n",
    "\n",
    "    # PCA (First Principal Component) on scaled training data\n",
    "    pca = PCA(n_components=1)\n",
    "    X_train_combined[\"combined_pca\"] = pca.fit_transform(\n",
    "        X_train_scaled_subset[top_targets]\n",
    "    )\n",
    "    X_test_combined[\"combined_pca\"] = pca.transform(X_test_scaled_subset[top_targets])\n",
    "\n",
    "    # LDA (First Linear Discriminant) on scaled training data\n",
    "    lda = LDA(n_components=1)\n",
    "    X_train_combined[\"combined_lda\"] = lda.fit_transform(\n",
    "        X_train_scaled_subset[top_targets], y_train_subset\n",
    "    )\n",
    "    X_test_combined[\"combined_lda\"] = lda.transform(X_test_scaled_subset[top_targets])\n",
    "\n",
    "    # For each combination method, calculate ROC analysis\n",
    "    roc_results_combined_raw[comparison] = (\n",
    "        {}\n",
    "    )  # Initialize empty dictionary for the comparison\n",
    "    roc_results_combined_scaled[comparison] = {}\n",
    "    for method in combined_methods:  # Using training and testing sets as before.\n",
    "        X_train_subset = X_train_combined[method].values.reshape(-1, 1)\n",
    "        X_test_subset = X_test_combined[method].values.reshape(-1, 1)\n",
    "        roc_results_combined_scaled[comparison][method] = roc_analysis(\n",
    "            X_train_subset,\n",
    "            X_test_subset,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            method,\n",
    "            comparison,\n",
    "            \"scaled\",\n",
    "        )  # Corrected analysis\n",
    "\n",
    "\n",
    "# Saving ROC Results (all methods together):\n",
    "roc_results_combined_all = {\n",
    "    \"combined_raw\": roc_results_combined_raw,\n",
    "    \"combined_scaled\": roc_results_combined_scaled,\n",
    "}\n",
    "\n",
    "# Iterate over each data type and create tables\n",
    "for data_type, results in roc_results_combined_all.items():\n",
    "    for comparison, values in results.items():\n",
    "        # Create directory for the ROC results if it doesn't exist\n",
    "        os.makedirs(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}\",\n",
    "            exist_ok=True,\n",
    "        )\n",
    "        temp = pd.DataFrame(values).transpose()\n",
    "        temp[\"method\"] = temp.index\n",
    "        temp.to_csv(\n",
    "            f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "import json\n",
    "\n",
    "# Save the entire dictionary to a JSON file\n",
    "with open(\"results/main/roc_results_combined_all.json\", \"w\") as fp:\n",
    "    json.dump(roc_results_combined_all, fp, indent=4)\n",
    "\n",
    "# 4. ROC Analysis (Top miRNAs and Clinical Parameters) will be added in the next response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc478bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (Previous ROC analysis code - Parts 1, 2 & 3) ...\n",
    "\n",
    "# 4. ROC Analysis (Top miRNAs and Clinical Parameters)\n",
    "\n",
    "# Select top clinical parameters based on correlation analysis\n",
    "# (This can be subjective; choose those with the strongest correlations)\n",
    "\n",
    "top_clinical_params = [\n",
    "    \"pocket_depth\",\n",
    "    \"bleeding_on_probing\",\n",
    "]  # Example  choose your top parameters\n",
    "\n",
    "# Combine top miRNAs and top clinical parameters for ROC analysis (using robust scaled data)\n",
    "roc_results_combined_clinical = {}\n",
    "\n",
    "for i in range(\n",
    "    1, len(groups)\n",
    "):  # Iterate through G and P groups (compare against Control 'S')\n",
    "    comparison_name = f\"S vs {groups[i]}\"\n",
    "\n",
    "    # Subset data for current comparison (training data only)\n",
    "    group1_indices = y_train[\"GROUP\"] == \"S\"  # Control group\n",
    "    group2_indices = y_train[\"GROUP\"] == groups[i]  # Current group (G or P)\n",
    "\n",
    "    X_train_scaled_subset = X_train_scaled[\n",
    "        group1_indices | group2_indices\n",
    "    ]  # Get scaled train data\n",
    "    y_train_subset = y_train[group1_indices | group2_indices][\"GROUP\"].apply(\n",
    "        lambda x: 0 if x == \"S\" else 1\n",
    "    )  # Create binary labels (0 for S, 1 for other)\n",
    "\n",
    "    roc_results_combined_clinical[comparison_name] = {}\n",
    "    # ROC Analysis using only Clinical Parameters\n",
    "    for param in top_clinical_params:\n",
    "        roc_results_combined_clinical[comparison_name][param] = roc_analysis(\n",
    "            X_train_scaled_subset[[param]],\n",
    "            X_test_scaled[test_subset][[param]],\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            param,\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "\n",
    "    # Combine Scaled Top miRNAs for the current comparison\n",
    "    combined_top_miRNA_train = (\n",
    "        X_train_scaled_subset[top_mirnas].mean(axis=1).values.reshape(-1, 1)\n",
    "    )\n",
    "    combined_top_miRNA_test = (\n",
    "        X_test_scaled[test_subset][top_mirnas].mean(axis=1).values.reshape(-1, 1)\n",
    "    )\n",
    "\n",
    "    for param in top_clinical_params:\n",
    "        # Combine top miRNAs and each top clinical parameter (Example - averaging method)\n",
    "        X_train_combined = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"combined_mirna\": combined_top_miRNA_train.flatten(),\n",
    "                    \"clinical_param\": X_train_scaled_subset[param],\n",
    "                }\n",
    "            )\n",
    "            .mean(axis=1)\n",
    "            .values.reshape(-1, 1)\n",
    "        )  # Averaging top miRNAs and current clinical parameter, using scaled data.\n",
    "\n",
    "        X_test_combined = (\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"combined_mirna\": combined_top_miRNA_test.flatten(),\n",
    "                    \"clinical_param\": X_test_scaled[test_subset][\n",
    "                        param\n",
    "                    ],  # Test data should be transformed with the parameters obtained from training.\n",
    "                }\n",
    "            )\n",
    "            .mean(axis=1)\n",
    "            .values.reshape(-1, 1)\n",
    "        )\n",
    "\n",
    "        combined_name = f\"combined_top_mirnas_{param}\"\n",
    "\n",
    "        roc_results_combined_clinical[comparison_name][combined_name] = roc_analysis(\n",
    "            X_train_combined,\n",
    "            X_test_combined,\n",
    "            y_train_subset,\n",
    "            y_test_subset,\n",
    "            combined_name,\n",
    "            comparison_name,\n",
    "            \"scaled\",\n",
    "        )\n",
    "\n",
    "# Save combined clinical ROC results\n",
    "# Create directory for the ROC results if it doesn't exist\n",
    "data_type = \"combined_clinical\"\n",
    "os.makedirs(f\"results/main/roc_{data_type}\", exist_ok=True)\n",
    "\n",
    "for comparison, values in roc_results_combined_clinical.items():\n",
    "    temp = pd.DataFrame(values).transpose()\n",
    "    temp[\"method\"] = temp.index\n",
    "    temp.to_csv(\n",
    "        f\"results/main/roc_{comparison.lower().replace(' ', '_')}_{data_type}.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
